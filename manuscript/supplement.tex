\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{chngcntr}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{lscape}
\usepackage{rotating}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

\numberwithin{equation}{section}
\numberwithin{table}{section}

\RequirePackage[OT1]{fontenc}
\usepackage{amsthm,amsfonts,amssymb,mathtools,tikz}
\usepackage{graphicx,bm,environ}
\usepackage{mathabx}
\usepackage{float}
\usepackage[boxed, linesnumbered,ruled,lined,commentsnumbered]{algorithm2e}
\RequirePackage{natbib}
\usepackage{xr-hyper}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[nameinlink]{cleveref}

% provide arXiv number if available:
%\arxiv{2101.07776}

% put your definitions there:
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newtheorem{ass}{Assumption}
\newcommand{\assautorefname}{Assumption}
\newtheorem{thm}{Theorem}
\numberwithin{thm}{section}
\newcommand{\thmautorefname}{Theorem}
\newtheorem{defn}{Definition}
\numberwithin{defn}{section}
\newcommand{\defnautorefname}{Definition}
\newtheorem{lem}{Lemma}
\numberwithin{lem}{section}
\newcommand{\lemautorefname}{Lemma}
\newtheorem{prop}{Proposition}
\numberwithin{prop}{section}
\newcommand{\propautorefname}{Proposition}
\newtheorem{cor}{Corollary}
\numberwithin{cor}{section}
\newcommand{\corautorefname}{Corollary}
\newtheorem{rem}{Remark}
\numberwithin{rem}{section}
\newcommand{\remautorefname}{Remark}
\renewcommand{\sectionautorefname}{Section}
\let\subsectionautorefname\sectionautorefname 	\let\subsubsectionautorefname\sectionautorefname
\renewcommand{\algorithmautorefname}{Algorithm}
\renewcommand{\thealgocf}{B.\arabic{algocf}}

\Crefname{ass}{Assumption}{Assumptions}
\Crefname{prop}{Proposition}{Propositions}
\Crefname{section}{Section}{Sections}
\Crefname{appendix}{Appendix}{Appendices}


\DeclareMathOperator*{\argmin}{arg min}
\DeclareMathOperator*{\argmax}{arg max}
\DeclareMathOperator{\Vector}{vec}
\DeclareMathOperator{\Vech}{vech}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\offvec}{offvec}
\DeclareMathOperator{\blkdiag}{blkdiag}
\DeclareMathOperator{\SNR}{SNR}
\DeclareMathOperator{\mat}{mat}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\diag}{diag}
\newcommand{\rank}{\operatorname{rk}} 
%\endlocaldefs

\newenvironment{ass*}
 {\expandafter\def\expandafter\theass\expandafter{\theass*}\ass}
 {\endass}
%\newtheorem{assumpB}{Assumption}
%\newcommand{\assumpBautorefname}{Assumption}
%\renewcommand\theassumpB{\arabic{assumpB}^{*}}
%\Crefname{assumpB}{Assumption}{Assumptions}


%\addbibresource{main.bib}

\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\newcommand*{\myexternaldocument}[1]{%
    \externaldocument[][nocite]{#1}%
    \addFileDependency{#1.tex}%
    \addFileDependency{#1.aux}%
}

\myexternaldocument{jasa-testing_simultaneous_diagonalizability}

\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


  \title{\LARGE\bf Supplement to ``Testing Simultaneous Diagonalizability"}
  \author{Yuchen Xu\thanks{Corresponding author: Yuchen Xu, Email address: \href{mailto:yx439@cornell.edu}{yx439@cornell.edu}}
    \and
    Marie-Christine D\"uker\thanks{Email address: \href{mailto:md2224@cornell.edu}{md2224@cornell.edu}}
    \and
    David S. Matteson\thanks{Email address: \href{mailto:matteson@cornell.edu}{matteson@cornell.edu}}
    \hspace{.7cm}\\
    Department of Statistics and Data Science, Cornell University}
  \maketitle

We present here supplementary results for the article “Testing Simultaneous Diagonalizability”. \autoref{se:compl} provides some additional numerical results. In \autoref{sec:llrt}, we propose and analyze an alternative to the commutator based test for the two-sample test problem.
\autoref{sec:ext} discusses an extension for symmetric matrices.
Finally, \Cref{app:profs,,app:est} contain all proofs.
We adopt the notation of the article and refer to its labels.


\begin{appendix}
\counterwithin{figure}{section}

\section{Complementary simulation results} \label{se:compl}
We provide here some empirical results complementary to the numerical analysis presented in the main paper. \autoref{se:compl1} gives empirical sizes and powers for the proposed test, 
\autoref{se:compl2} studies sequential application of our partial tests and \autoref{se:compl3} discusses application to possibly high-dimensional data.

\subsection{Empirical Type I and II errors} \label{se:compl1}
In addition to the p-values in the main paper, we provide here tables with Type I and II errors for our tests to assess their performances. \Cref{ta:errorsTwosample,,ta:errorsMultisample,,ta:errorsPartial} show respectively the errors for the two-sample, multi-sample and partial tests.

\begin{table}[htbp] 
\centering
\begin{tabular}{|c|c|c|c|cc|}
\hline
\multirow{2}{*}{Test Type} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Statistics\\ Type\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Sample\\ Size\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Type I\\ Error\end{tabular}} & \multicolumn{2}{c|}{Type II Error} \\ \cline{5-6} 
 &  &  &  & \multicolumn{1}{c|}{SNR=1000} & SNR=10 \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Commutator-based\\ test\end{tabular}} & \multirow{3}{*}{Chi-test} & 50 & 0.218 & \multicolumn{1}{c|}{0.216} & 0.000 \\ \cline{3-6} 
 &  & 250 & 0.056 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{3-6} 
 &  & 1000 & 0.054 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \hline
\multirow{12}{*}{\begin{tabular}[c]{@{}c@{}}LLR test\\ (\autoref{sec:llrt})\end{tabular}} & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Oracle Chi-Test\\ with \eqref{eqn:polyP}\end{tabular}} & 50 & 0.182 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{3-6} 
 &  & 250 & 0.140 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{3-6} 
 &  & 1000 & 0.060 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Plugin Chi-test\\ with \eqref{eqn:polyP}\end{tabular}} & 50 & 1.000 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{3-6} 
 &  & 250 & 1.000 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{3-6} 
 &  & 1000 & 1.000 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Oracle Chi-Test\\ with \eqref{eqn:eigvP}\end{tabular}} & 50 & 0.182 & \multicolumn{1}{c|}{0.058} & 0.000 \\ \cline{3-6} 
 &  & 250 & 0.140 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{3-6} 
 &  & 1000 & 0.060 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Plugin Chi-test\\ with \eqref{eqn:eigvP}\end{tabular}} & 50 & 0.760 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{3-6} 
 &  & 250 & 0.842 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{3-6} 
 &  & 1000 & 0.774 & \multicolumn{1}{c|}{0.000} & 0.000 \\ \hline
\end{tabular}
\caption{Two-sample test results on simulated $\mathcal{M}_2(\rho, 5; 5)$ for $\rho^2 = \frac{1}{\SNR} \in \{ 0, \frac{1}{1000}, \frac{1}{10} \}$.} \label{ta:errorsTwosample}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|ccc|}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Statistics\\ Type\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Sample\\ Size\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Type I\\ Error\end{tabular}} & \multicolumn{3}{c|}{Type II Error} \\ \cline{4-6} 
 &  &  & \multicolumn{1}{c|}{$\SNR=1000$} & \multicolumn{1}{c|}{$\SNR=100$} & $\SNR=10$ \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Oracle\\ Chi-test\\(\autoref{thm:multi_eig})\end{tabular}} & 100 & 0.230 & \multicolumn{1}{c|}{NA} & \multicolumn{1}{c|}{NA} & NA \\ \cline{2-6} 
 & 1000 & 0.060 & \multicolumn{1}{c|}{NA} & \multicolumn{1}{c|}{NA} & NA \\ \cline{2-6} 
 & 10000 & 0.045 & \multicolumn{1}{c|}{NA} & \multicolumn{1}{c|}{NA} & NA \\ \cline{2-6} 
 & 100000 & 0.070 & \multicolumn{1}{c|}{NA} & \multicolumn{1}{c|}{NA} & NA \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Plugin\\ Chi-test\\(\autoref{thm:multi_eig.Est})\end{tabular}} & 100 & 0.175 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & 1000 & 0.095 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & 10000 & 0.090 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & 100000 & 0.075 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Plugin\\ Gamma-test\\(\autoref{cor:multi})\end{tabular}} & 100 & 0.015 & \multicolumn{1}{c|}{0.005} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & 1000 & 0.025 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & 10000 & 0.005 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & 100000 & 0.015 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \hline
\end{tabular}
\caption{Multi-sample test results on simulated $\mathcal{M}_8(\rho, 4; 4)$ for $\rho^2 = \frac{1}{\SNR} \in \{ 0, \frac{1}{1000}, \frac{1}{100}\, \frac{1}{10} \}$.} \label{ta:errorsMultisample}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|ccc|}
\hline
\multirow{2}{*}{Statistics Type} & \multirow{2}{*}{Sample Size} & \multirow{2}{*}{Type I Error} & \multicolumn{3}{c|}{Type II Error} \\ \cline{4-6} 
 &  &  & \multicolumn{1}{c|}{$\SNR=1000$} & \multicolumn{1}{c|}{$\SNR=100$} & $\SNR=10$ \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Chi-test\\(\autoref{thm:part.Est})\end{tabular}} & 100 & 0.020 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & 1000 & 0.020 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & 10000 & 0.025 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Gamma-test\\(\autoref{cor:partial})\end{tabular}} & 100 & 0.010 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & 1000 & 0.015 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \cline{2-6} 
 & 10000 & 0.015 & \multicolumn{1}{c|}{0.000} & \multicolumn{1}{c|}{0.000} & 0.000 \\ \hline
\end{tabular}
\caption{Partial test results on simulated $\mathcal{M}_8(\rho, 2; 4)$ for $\rho = \frac{1}{\SNR} \in \{ 0, \frac{1}{1000}, \frac{1}{100}\, \frac{1}{10} \}$.} \label{ta:errorsPartial}
\end{table}

\subsection{Sequential application of partial tests} \label{se:compl2}
As pointed out in \autoref{sec:part}, we assume that the number of partial common eigenvectors in known. Since this assumption is not feasible in practice, we propose a sequential testing procedure. The hypothesis testing problem \eqref{hyp:part} can be stated for $k \in \{1,\dots,d\}$. The sequential testing starts with $k = d$, then $k = d-1$ and so on, till the null hypothesis is not rejected.
The performance of this procedure is accessed through a simulation study in \autoref{ta:sequential}. 


\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|ccc|}
\hline
\multirow{2}{*}{Statistics Type} & \multirow{2}{*}{Sample Size} & \multicolumn{3}{c|}{Rejection Rate} \\ \cline{3-5} 
 &  & \multicolumn{1}{c|}{$k=2$} & \multicolumn{1}{c|}{$k=3$} & $k=4$ \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Chi-test\\(\autoref{thm:part.Est})\end{tabular}} & 100 & \multicolumn{1}{c|}{0.020} & \multicolumn{1}{c|}{1.000} & 1.000 \\ \cline{2-5} 
 & 1000 & \multicolumn{1}{c|}{0.020} & \multicolumn{1}{c|}{1.000} & 1.000 \\ \cline{2-5} 
 & 10000 & \multicolumn{1}{c|}{0.025} & \multicolumn{1}{c|}{1.000} & 1.000 \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Gamma-test\\(\autoref{cor:partial})\end{tabular}} & 100 & \multicolumn{1}{c|}{0.010} & \multicolumn{1}{c|}{1.000} & 1.000 \\ \cline{2-5} 
 & 1000 & \multicolumn{1}{c|}{0.015} & \multicolumn{1}{c|}{1.000} & 1.000 \\ \cline{2-5} 
 & 10000 & \multicolumn{1}{c|}{0.015} & \multicolumn{1}{c|}{1.000} & 1.000 \\ \hline
\end{tabular}
\caption{Partial test results on simulated $\mathcal{M}_8(0, 2; 4)$ and potentially mis-specified $k \in \{2,3,4\}$.} \label{ta:sequential}
\end{table}

\subsection{High-dimensional data} \label{se:compl3}

In this work, we consider the classical ``fixed $d$, large $n$” regime. However, many contemporary data go beyond the low dimensional setting and require the dimension $d$ to be of the same order as, or possibly even larger than, the sample size $n$.
While the high-dimensional setting goes beyond the scope of this work, we would like to point out why our methodology is not sufficient to do testing on high-dimensional data.

\autoref{ta:differentdimensions} presents the empirical rejection rates and average degrees of freedom for the two-sample test in \autoref{thm:comm}, considering different sample sizes $n=50, 100, 500$ and letting $d$ grow. We present results assuming that the limiting covariance matrix in \eqref{eq:comm.asym} is estimated and known. The existence of a consistent estimator is stated in \autoref{ass:covConsistent} and makes our procedure feasible in practice.

The results in \autoref{ta:differentdimensions} show that the classical theory suffers a $\alpha$ test size much higher than the nominal test level once we consider high-dimensional data and estimate the limiting covariance matrix. Intuitively, the results are expected to break down once the sample size does not satisfy $n > r_{1}(d^2 + d^2)$. This can be easily seen by counting the degrees of freedom required to specify a rank-$r_{1}$ matrix of size $d^2 \times d^2$. Roughly speaking, we need $r_{1}$ numbers to specify the matrix's singular values, and $r_{1}d^2$ and $r_{1}d^2$ numbers to specify its left and right singular vectors.

The $\alpha$ test size much higher than the nominal test level is also due to \autoref{ass:covConsistent} no longer being satisfied in a high-dimensional regime. In particular, the difference between estimator and true matrix is incorrectly normalized once the dimension grows with the sample size. It is expected to require results from random matrix theory to get convergence under suitable assumptions on the ratio between $d$ and $n$. 


\begin{sidewaystable}[htbp]
\centering
  \small
\begin{tabular}{|c|cccc|cccc|cccc|}
\hline
\multirow{3}{*}{$d$} & \multicolumn{4}{c|}{Sample Size=50} & \multicolumn{4}{c|}{Sample Size=100} & \multicolumn{4}{c|}{Sample Size = 500} \\ \cline{2-13} 
 & \multicolumn{2}{c|}{Empirical Cov} & \multicolumn{2}{c|}{True Cov} & \multicolumn{2}{c|}{Empirical Cov} & \multicolumn{2}{c|}{True Cov} & \multicolumn{2}{c|}{Empirical Cov} & \multicolumn{2}{c|}{True Cov} \\ \cline{2-13} 
 & \multicolumn{1}{c|}{Size} & \multicolumn{1}{c|}{Avg DF} & \multicolumn{1}{c|}{Size} & Avg DF & \multicolumn{1}{c|}{Size} & \multicolumn{1}{c|}{Avg DF} & \multicolumn{1}{c|}{Size} & Avg DF & \multicolumn{1}{c|}{Size} & \multicolumn{1}{c|}{Avg DF} & \multicolumn{1}{c|}{Size} & Avg DF \\ \hline
2 & \multicolumn{1}{c|}{0.036} & \multicolumn{1}{c|}{2.05} & \multicolumn{1}{c|}{0.022} & 2.04 & \multicolumn{1}{c|}{0.054} & \multicolumn{1}{c|}{2.00} & \multicolumn{1}{c|}{0.058} & 2.00 & \multicolumn{1}{c|}{0.044} & \multicolumn{1}{c|}{2.00} & \multicolumn{1}{c|}{0.044} & 2.00 \\ \hline
3 & \multicolumn{1}{c|}{0.066} & \multicolumn{1}{c|}{6.10} & \multicolumn{1}{c|}{0.034} & 6.13 & \multicolumn{1}{c|}{0.074} & \multicolumn{1}{c|}{6.02} & \multicolumn{1}{c|}{0.054} & 6.02 & \multicolumn{1}{c|}{0.062} & \multicolumn{1}{c|}{6.00} & \multicolumn{1}{c|}{0.060} & 6.00 \\ \hline
4 & \multicolumn{1}{c|}{0.088} & \multicolumn{1}{c|}{12.33} & \multicolumn{1}{c|}{0.014} & 12.38 & \multicolumn{1}{c|}{0.092} & \multicolumn{1}{c|}{12.06} & \multicolumn{1}{c|}{0.036} & 12.06 & \multicolumn{1}{c|}{0.066} & \multicolumn{1}{c|}{12.00} & \multicolumn{1}{c|}{0.054} & 12.00 \\ \hline
5 & \multicolumn{1}{c|}{0.186} & \multicolumn{1}{c|}{20.15} & \multicolumn{1}{c|}{0.018} & 20.42 & \multicolumn{1}{c|}{0.116} & \multicolumn{1}{c|}{20.05} & \multicolumn{1}{c|}{0.050} & 20.08 & \multicolumn{1}{c|}{0.062} & \multicolumn{1}{c|}{20.00} & \multicolumn{1}{c|}{0.056} & 20.00 \\ \hline
6 & \multicolumn{1}{c|}{0.358} & \multicolumn{1}{c|}{29.75} & \multicolumn{1}{c|}{0.020} & 30.65 & \multicolumn{1}{c|}{0.176} & \multicolumn{1}{c|}{29.97} & \multicolumn{1}{c|}{0.020} & 30.09 & \multicolumn{1}{c|}{0.058} & \multicolumn{1}{c|}{30.00} & \multicolumn{1}{c|}{0.046} & 30.00 \\ \hline
7 & \multicolumn{1}{c|}{0.698} & \multicolumn{1}{c|}{40.85} & \multicolumn{1}{c|}{0.018} & 42.79 & \multicolumn{1}{c|}{0.310} & \multicolumn{1}{c|}{40.46} & \multicolumn{1}{c|}{0.026} & 40.97 & \multicolumn{1}{c|}{0.092} & \multicolumn{1}{c|}{40.00} & \multicolumn{1}{c|}{0.050} & 40.00 \\ \hline
8 & \multicolumn{1}{c|}{0.960} & \multicolumn{1}{c|}{55.82} & \multicolumn{1}{c|}{0.024} & 57.78 & \multicolumn{1}{c|}{0.548} & \multicolumn{1}{c|}{56.04} & \multicolumn{1}{c|}{0.034} & 56.30 & \multicolumn{1}{c|}{0.092} & \multicolumn{1}{c|}{56.00} & \multicolumn{1}{c|}{0.044} & 56.00 \\ \hline
9 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{69.11} & \multicolumn{1}{c|}{0.014} & 74.19 & \multicolumn{1}{c|}{0.778} & \multicolumn{1}{c|}{70.54} & \multicolumn{1}{c|}{0.044} & 71.97 & \multicolumn{1}{c|}{0.098} & \multicolumn{1}{c|}{70.44} & \multicolumn{1}{c|}{0.052} & 70.57 \\ \hline
10 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{86.27} & \multicolumn{1}{c|}{0.020} & 92.94 & \multicolumn{1}{c|}{0.962} & \multicolumn{1}{c|}{89.62} & \multicolumn{1}{c|}{0.034} & 90.74 & \multicolumn{1}{c|}{0.162} & \multicolumn{1}{c|}{90.00} & \multicolumn{1}{c|}{0.054} & 90.00 \\ \hline
11 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{96.93} & \multicolumn{1}{c|}{0.024} & 114.55 & \multicolumn{1}{c|}{0.996} & \multicolumn{1}{c|}{108.25} & \multicolumn{1}{c|}{0.030} & 111.15 & \multicolumn{1}{c|}{0.238} & \multicolumn{1}{c|}{108.79} & \multicolumn{1}{c|}{0.050} & 109.15 \\ \hline
12 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.030} & 136.88 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{131.96} & \multicolumn{1}{c|}{0.042} & 133.48 & \multicolumn{1}{c|}{0.340} & \multicolumn{1}{c|}{132.00} & \multicolumn{1}{c|}{0.056} & 132.00 \\ \hline
13 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.022} & 161.63 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{153.87} & \multicolumn{1}{c|}{0.030} & 157.93 & \multicolumn{1}{c|}{0.468} & \multicolumn{1}{c|}{156.00} & \multicolumn{1}{c|}{0.044} & 156.00 \\ \hline
14 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.028} & 188.91 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{174.85} & \multicolumn{1}{c|}{0.026} & 184.56 & \multicolumn{1}{c|}{0.622} & \multicolumn{1}{c|}{182.00} & \multicolumn{1}{c|}{0.044} & 182.00 \\ \hline
15 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.040} & 217.63 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{193.89} & \multicolumn{1}{c|}{0.042} & 212.96 & \multicolumn{1}{c|}{0.774} & \multicolumn{1}{c|}{209.88} & \multicolumn{1}{c|}{0.052} & 209.99 \\ \hline
16 & \multicolumn{1}{c|}{0.998} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.042} & 248.27 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{198.00} & \multicolumn{1}{c|}{0.034} & 243.25 & \multicolumn{1}{c|}{0.874} & \multicolumn{1}{c|}{239.89} & \multicolumn{1}{c|}{0.042} & 240.00 \\ \hline
17 & \multicolumn{1}{c|}{0.998} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.050} & 281.03 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{198.00} & \multicolumn{1}{c|}{0.042} & 275.62 & \multicolumn{1}{c|}{0.974} & \multicolumn{1}{c|}{270.00} & \multicolumn{1}{c|}{0.046} & 270.14 \\ \hline
18 & \multicolumn{1}{c|}{0.978} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.050} & 315.85 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{198.00} & \multicolumn{1}{c|}{0.036} & 310.37 & \multicolumn{1}{c|}{0.986} & \multicolumn{1}{c|}{303.18} & \multicolumn{1}{c|}{0.056} & 304.74 \\ \hline
19 & \multicolumn{1}{c|}{0.994} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.046} & 352.90 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{198.00} & \multicolumn{1}{c|}{0.050} & 347.06 & \multicolumn{1}{c|}{0.998} & \multicolumn{1}{c|}{340.60} & \multicolumn{1}{c|}{0.044} & 341.50 \\ \hline
20 & \multicolumn{1}{c|}{0.994} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.056} & 391.32 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{198.00} & \multicolumn{1}{c|}{0.046} & 384.37 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{373.18} & \multicolumn{1}{c|}{0.044} & 375.67 \\ \hline
21 & \multicolumn{1}{c|}{0.460} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.054} & 433.27 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{198.00} & \multicolumn{1}{c|}{0.040} & 426.84 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{418.44} & \multicolumn{1}{c|}{0.044} & 419.91 \\ \hline
22 & \multicolumn{1}{c|}{0.982} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.044} & 475.39 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{198.00} & \multicolumn{1}{c|}{0.040} & 468.52 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{455.78} & \multicolumn{1}{c|}{0.054} & 459.21 \\ \hline
23 & \multicolumn{1}{c|}{0.986} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.044} & 520.58 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{198.00} & \multicolumn{1}{c|}{0.054} & 513.53 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{499.67} & \multicolumn{1}{c|}{0.042} & 503.17 \\ \hline
24 & \multicolumn{1}{c|}{0.284} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.032} & 567.74 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{198.00} & \multicolumn{1}{c|}{0.030} & 560.31 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{544.63} & \multicolumn{1}{c|}{0.040} & 551.13 \\ \hline
25 & \multicolumn{1}{c|}{0.702} & \multicolumn{1}{c|}{98.00} & \multicolumn{1}{c|}{0.056} & 617.31 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{198.00} & \multicolumn{1}{c|}{0.046} & 609.14 & \multicolumn{1}{c|}{1.000} & \multicolumn{1}{c|}{590.03} & \multicolumn{1}{c|}{0.054} & 595.95 \\ \hline
\end{tabular}
\caption{Two-sample test results on simulated $\mathcal{M}_2(0, d; d)$ for dimensions $d \in \{ 2, \dots, 25 \}$.} \label{ta:differentdimensions}
\label{ta:highd}
\end{sidewaystable}

\section{Log-likelihood Ratio (LLR) test framework}
\label{sec:llrt}

In the main paper, we studied a commutator based two-sample test. In this section, we propose an alternative test based on a likelihood ratio test statistic.

Before we introduce the test statistic we state an assumption which is slightly stronger than \autoref{ass:distinct} in the main paper.

\setcounter{ass}{2}
\begin{ass*}\label{ass:distinct2}
Each $M_i$, $i=1,\dots,p$, has $d$ distinct non-zero real eigenvalues.
\end{ass*}





According to the assumed asymptotic normality in \autoref{ass:normal}, we introduce the log-likelihood type function for the estimators $A_1$ and $A_2$ as
\begin{equation}\label{eqn:loglike}
    L(M_1, M_2) := - \sum_{i = 1}^2 \Vector(A_i - M_i)' \Sigma_i^+ \Vector(A_i - M_i).
\end{equation}
It is then possible to obtain the supremum of $L(M_1, M_2)$ within the parameter spaces $H_0$ and $H_0 \cup H_1$, respectively, as
\begin{equation}\label{eqn:llrMLE}
    \widetilde{L}_0 := \sup_{(M_1, M_2) \in H_0} L(M_1, M_2), ~ ~ \widetilde{L}_1 := \sup_{(M_1, M_2) \in H_0 \cup H_1} L(M_1, M_2).
\end{equation}
In particular, we introduce a new version of M-estimators for $(M_1, M_2)$ under the null hypothesis $H_0$ as
\begin{equation}
    (\widehat{A}_1, \widehat{A}_2) = \argmax_{(M_1, M_2) \in H_0} L(M_1, M_2),\label{eqn:Mest}
\end{equation}
and the design of the ratio-test statistic can be given by
$
\Gamma_2 ~ \propto ~ - (\widetilde{L}_0 - \widetilde{L}_1) = - \widetilde{L}_0.
$

Indeed the estimators $\widehat{A}_1$ and $\widehat{A}_2$ in \eqref{eqn:Mest} can be explicitly computed given \autoref{ass:distinct2}. We introduce the following proposition and prove it in \autoref{app:profs}.

\begin{prop}\label{thm:ahat}
Suppose \autoref{ass:distinct2}.
Then, the optimizer $(\widehat{A}_1,\widehat{A}_2)$ that maximizes \eqref{eqn:loglike} under $H_0$ is given by
\begin{equation}\label{eqn:Ahat}
    \begin{aligned}
        & \Vector(\widehat{A}_1) = P_2 (P_2' \Sigma_1^+ P_2)^+ P_2' \Sigma_1^+ \Vector(A_1),\
        & \Vector(\widehat{A}_2) = P_1 (P_1' \Sigma_2^+ P_1)^+ P_1' \Sigma_2^+ \Vector(A_2),
    \end{aligned}
\end{equation}
with $P_i$ for $i = 1, 2$ generated from either of the two following setups:\begin{itemize}
    \item Polynomial basis: with $M_i^j$ the $j$-th power of $M_i$, for $i = 1, 2$,
    \begin{equation}\label{eqn:polyP}
        P_i = (\Vector(M_i^1)/\|M_i^1\|_F, \dots, \Vector(M_i^{d})/\|M_i^d\|_F);
    \end{equation}
    \item Eigenvector basis: with $V = (v_1, \dots, v_d) \in \mathbb{R}^{d \times d}$ the common eigenvectors of $M_1$ and $M_2$, $U = (u_1, \dots, u_d)' = V^{-1}$,
    \begin{equation}\label{eqn:eigvP}
        P_1 = P_2 = (\Vector(v_1 u_1'), \dots, \Vector(v_d u_d')).
    \end{equation}
\end{itemize}
\end{prop}

\subsection{LLR test statistic}\label{sec:llrintro}
In this section we introduce the LLR test statistic and provide its asymptotic behavior.
Under ideal conditions such that the true matrices $M_1$ and $M_2$ are known, we introduce the LLR test statistic
\begin{equation}\label{eqn:gamma1}
\begin{aligned}
    \Gamma_2 := c^2(n) \Big[ & \Vector(A_1 - \widehat{A}_1)' \Sigma_1^+ \Vector(A_1 - \widehat{A}_1) + \Vector(A_2 - \widehat{A}_2)' \Sigma_2^+ \Vector(A_2 - \widehat{A}_2)\Big]\\
    = c^2(n) \Big[ & \Vector (A_1)' Q_{1,2} \Vector(A_1) + \Vector(A_2)' Q_{2,1} \Vector(A_2)\Big],
\end{aligned}
\end{equation}
where $Q_{k,\ell} = \Sigma_k^+ - \Sigma_k^+ P_\ell (P_\ell' \Sigma_k^+ P_\ell)^+ P_\ell' \Sigma_k^+$ for $k, \ell = 1, 2$ and $k \ne \ell$,
and present its asymptotic behavior in the following proposition.
\begin{prop}[LLR test statistic]\label{thm:LLR}
Suppose \Cref{ass:normal,ass:distinct2} are satisfied. Then, under $H_0$ in \eqref{hyp:all}, the test statistic $\Gamma_2$ in \eqref{eqn:gamma1} satisfies
\begin{equation}\label{eqn:projTest}
    \Gamma_2 \xrightarrow{\mathcal{D}} \chi^2(r_2),
\end{equation}
where $r_2 = r_{1,2} + r_{2,1}$ and $r_{k,\ell} = \rank(\Sigma_k) - \rank(P_{\ell}' \Sigma_k^+ P_{\ell})$ for $k, \ell = 1,2$, and $k \neq \ell$.
\end{prop}

%\subsubsection{A practicable version of the LLR statistic}\label{sec:estLLR}

Note that when $\Sigma_1$ and $\Sigma_2$ are non-singular, $r_2 = 2d^2 - 2d$.
With our loose constraints on the covariance matrices, we may encounter singularity issues when computing \eqref{eqn:gamma1} with $\Sigma_1^+$ and $\Sigma_2^+$. To have a tractable version of \autoref{thm:LLR} with respect to the limiting covariance matrices, we propose to use the truncated version \eqref{eq:wald.cmt}. Note that the generalized inverse of $P_\ell' \Sigma_k^+ P_\ell$ is a part of a projection matrix hence will not have the same discontinuity concerns.

%While intuitively using the estimators $\widehat{\Sigma}_i$ of $\Sigma_i$ for $i = 1,2$ in \autoref{thm:LLR}, there can be serious issues due to the discontinuity of the Moore-Penrose general inverse and the rank when $\Sigma_i$ are singular. For this reason, the continuous mapping theorem is no longer applicable. Referring to \cite{lutkepohl1997modified}, we again apply the truncated SVD approximations and introduce the following proposition to deal with the potential inconsistency.

\begin{prop}\label{thm:LLRest}
Suppose \Cref{ass:normal,,ass:covConsistent,,ass:distinct2} are satisfied. Let $\varepsilon > 0$ be a threshold that is not an eigenvalue of $\Sigma_1$ and $\Sigma_2$. Define the test statistic
\begin{equation}\label{eqn:gamma1.Est}
    \Gamma_2^\#(\varepsilon) := c^2(n) \Big[\Vector (A_1)' \widehat{Q}_{1,2}[\varepsilon] \Vector(A_1) + \Vector(A_2)' \widehat{Q}_{2,1}[\varepsilon] \Vector(A_2)\Big]
\end{equation}
with
\begin{equation} \label{eq:Qkl}
\widehat{Q}_{k,\ell}[\varepsilon] := \widehat{\Sigma}_k^+(\varepsilon) - \widehat{\Sigma}_k^+(\varepsilon) P_\ell \big(P_\ell' \widehat{\Sigma}_k^+(\varepsilon) P_\ell\big)^+ P_\ell' \widehat{\Sigma}_k^+(\varepsilon)
\end{equation}
for $k, \ell = 1, 2$, and $k \neq \ell$. Then,
\begin{equation}\label{eq:projTest.Est}
    \Gamma_2^\#(\varepsilon) \xrightarrow{\mathcal{D}} \xi, \hspace{0.15cm} \text{ with } \hspace{0.15cm} \xi \sim \chi^2\big(\widehat{r}_2(\varepsilon)\big),
\end{equation}
where $\widehat{r}_2(\varepsilon) = \widehat{r}_{1,2}(\varepsilon) + \widehat{r}_{2,1}(\varepsilon)$ and $\widehat{r}_{k,\ell}(\varepsilon) = \rank(\widehat{\Sigma}_k; \varepsilon) - \rank\big( P_{\ell}' \widehat{\Sigma}_k^+(\varepsilon) P_{\ell} \big)$ for $k, \ell = 1,2$, and $k \neq \ell$. Furthermore, note that $\widehat{r}_2^l(\varepsilon) \leq \widehat{r}_2(\varepsilon) \leq \widehat{r}_2^u(\varepsilon)$ with
$$
\widehat{r}_2^l(\varepsilon) := \rank(\widehat{\Sigma}_1; \varepsilon) + \rank(\widehat{\Sigma}_2; \varepsilon) - 2d, \hspace{0.25cm} \widehat{r}_2^u(\varepsilon) := \rank(\widehat{\Sigma}_1; \varepsilon) + \rank(\widehat{\Sigma}_2; \varepsilon).
$$
Then, with $\xi^l \sim \chi^2(\widehat{r}^l_2(\varepsilon)), ~ \xi^u \sim \chi^2(\widehat{r}^u_2(\varepsilon))$, the p-value based on \eqref{eq:projTest.Est} can be bounded by
\begin{equation}\label{eq:upperBound}
    \mathbb{P} \big( \xi^l > \Gamma_2^\#(\varepsilon) ~|~ H_0\big) \leq \mathbb{P} \big( \xi > \Gamma_2^\#(\varepsilon) ~|~ H_0\big) \leq \mathbb{P} \big( \xi^u > \Gamma_2^\#(\varepsilon) ~|~ H_0\big).
\end{equation}
% In addition, the threshold $\varepsilon$ can be optimally chosen to satisfy $\varepsilon = o(1)$ and $\sigma(n)/\varepsilon = o(1)$ as $n \to \infty$.
\end{prop}

We include \eqref{eq:upperBound} to deal with the potentially inconsistent rank estimators of $P_{\ell}' \widehat{\Sigma}_k^+(\varepsilon) P_{\ell}$, $k, \ell = 1,2$ with $k \neq \ell$, and state the following proposition to justify the effectiveness of the relaxed test based on \eqref{eq:upperBound}. In particular, the proposition indicates that the hypothesis gets rejected with high probability within the hypothesis space $H_{1}$.

\begin{prop}\label{prop:alt.behave}
Under the alternative hypothesis $H_1$ in \eqref{hyp:part}, set
\begin{align*}
	& \bm{m}_1 = \Vector (M_1) - P_2 \big(P_2' \Sigma_1^+(\varepsilon) P_2\big)^+ P_2' \Sigma_1^+(\varepsilon) \Vector(M_1),\\
	& \bm{m}_2 = \Vector (M_2) - P_1\big(P_1' \Sigma_2^+(\varepsilon) P_1\big)^+ P_1' \Sigma_2^+(\varepsilon) \Vector(M_2),
\end{align*}
with $\varepsilon$ chosen by \autoref{thm:LLRest} and $\bm{m}_i \in \mathbb{R}^{d^2}$ for $i = 1,2$. If $\widehat{\Sigma}_i^+(\varepsilon) \bm{m}_i \neq 0$ for $i = 1,2$, then the test statistic \eqref{eqn:gamma1.Est} satisfies
$$
\lim_{n \to \infty} \Gamma_2^\#(\varepsilon) \to \infty \hspace{0.15cm} \Rightarrow \hspace{0.15cm} \lim_{n \to \infty} \mathbb{P} \big( \xi > \Gamma_2^\#(\varepsilon) ~|~ H_1\big) = 0.
$$
\end{prop}

Note that under the null hypothesis $H_0$, it might also be true that $\bm{m}_i \neq 0$ when $\widehat{\Sigma}_i^+(\varepsilon)$ is singular, but $\widehat{\Sigma}_i^+(\varepsilon) \bm{m}_i = 0$ for $i = 1,2$ always hold.



\subsection{Error analysis}
\label{sec:error}
In this section, we study the effects of replacing the matrices $P_1$ and $P_2$ in \eqref{eqn:Ahat} by their estimators in our proposed test. In particular, we start from the expression \eqref{eqn:polyP} of polynomial basis. We define the estimators \eqref{eqn:polyP} for $P_1$ and $P_2$ as
$$
\widehat{P}_i = (\Vector(A_i^1)/\|A_i^1\|_F, \dots, \Vector(A_i^{d})/\|A_i^d\|_F)
$$
for $i = 1,2$. Then, under \autoref{ass:normal}, $\widehat{P}_1$ and $\widehat{P}_2$ are consistent estimators for $P_1$ and $P_2$ with the same convergence rate $1/c(n)$. However, even with extra care about the covariance singularity, replacing $P_i$ by $\widehat{P}_i$, for $i = 1, 2$, in $\Gamma_2^\#(\varepsilon)$ in \eqref{eqn:gamma1.Est} makes the asymptotic distribution of the test statistic \eqref{eq:projTest.Est} inaccurate. For this reason, one thing remains to be discussed is whether the statistical order of the error introduced from this approximation step is negligible in testing. To be more precise, for \autoref{thm:LLRest}, the error for the first summand in $\Gamma_2^\#(\varepsilon)$ is
\begin{equation} \label{eqn:deltaeps}
    \Delta_\varepsilon := 
    c^2(n) \Vector (A_1)' (\widehat{Q}_{1,2}[\varepsilon] - \widehat{\mathcal{Q}}_{1,2}[\varepsilon]) \Vector(A_1) 
\end{equation}
with $\widehat{Q}_{1,2}[\varepsilon]$ as in \eqref{eq:Qkl} and $\widehat{\mathcal{Q}}_{1,2}[\varepsilon]$ is defined by replacing the matrices $P_{\ell}$ in \eqref{eq:Qkl} by their sample counterparts $\widehat{P}_{\ell}$ such that
\begin{equation} \label{eqn:mathcalQ}
\widehat{\mathcal{Q}}_{k,\ell}[\varepsilon] := \widehat{\Sigma}_k^+(\varepsilon) - \widehat{\Sigma}_k^+(\varepsilon) \widehat{P}_\ell \big(\widehat{P}_\ell' \widehat{\Sigma}_k^+(\varepsilon) \widehat{P}_\ell\big)^+ \widehat{P}_\ell' \widehat{\Sigma}_k^+(\varepsilon)
\end{equation}
for $k, \ell = 1, 2$, $k \neq \ell$. The following proposition provides information about the asymptotic behavior of $\Delta_\varepsilon$ in \eqref{eqn:deltaeps}. The proof can be found in \autoref{app:profs}.

\begin{prop}\label{lem:asyerror} 
Assume the choice of $\varepsilon$ satisfies $\Sigma_1(\varepsilon) = \Sigma_1$. Then, under \autoref{ass:normal}, there exists an $r_{1,2} \times r_{1,2}$ positive semi-definite matrix
$
\widecheck{\Sigma} = \widecheck{\Sigma}(M_1, M_2, \Sigma_1, \Sigma_2)
$
such that the error term in \eqref{eqn:deltaeps} satisfies
$$
\Delta_\varepsilon \xrightarrow{\mathcal{D}} 
Z, \hspace{0.12cm} \text{ with } \hspace{0.12cm} Z= 
\sum_{i = 1}^{r_{1,2}} 2\sqrt{\lambda_i} (\nu_{i,1} - \nu_{i,2}),
$$
where $\lambda_i$ are the eigenvalues of $\widecheck{\Sigma}$, and $\nu_{i,j} \stackrel{i.i.d.}{\sim} \chi^2(1)$ for $i \in  \{1, \dots, r_{1,2}\}, ~ j=1, 2$. Furthermore, the variance of the limit is $\var(Z) = 16 \tr(\widecheck{\Sigma})$.
\end{prop}

According to \autoref{lem:asyerror}, the error term $\Delta_\varepsilon$ in \eqref{eqn:delta1} is still asymptotically unbiased. However, with a mild choice of matrix dimension $d$, its asymptotic variance, which represents the perturbation range, is comparable with the magnitude of the test statistic $\Gamma_2^\#(\varepsilon)$ in \eqref{eq:projTest.Est}, as the matrix $\widecheck{\Sigma} \in \mathbb{R}^{r_{1,2} \times r_{1,2}}$ is generated by well-conditioned matrices $(M_1, M_2, \Sigma_1, \Sigma_2)$. Hence, even with the relaxed test introduced in \autoref{thm:LLRest}, there are no guarantees that the test statistic is valid in real applications. The weighted projections $\widehat{A}_i$, however, could sometimes be useful while problem setup or interests change.

On the other hand, due to the lack of stochastic convergence results for optimization with respect to the common eigenvectors $V$, the consistency rate of plugging $\widehat{V}$ from `\textit{(W)JDTE}' along with its inverse $\widehat{U} = \widehat{V}^{-1}$ into \eqref{eqn:eigvP} for $\widehat{P}_i$ remains unclear. However, as long as the optimization procedure fails to improve the original $1/c(n)$ rate in \Cref{ass:normal} with positive probability, the analogous derivations will lead to a similar conclusion as \autoref{lem:asyerror}.

%In fact, if with the knowledge of common eigenstructures, say, the eigenvectors are known or a reference square matrix is exactly provided to possess the shared eigenvectors, one can simply re-define the space matrices $P_1$ and $P_2$ using such prior information to make this particular approach applicable with reasonably strong test power.

On the contrary, if one has confident prior knowledge of common eigen-structures, one can simply define the space matrices $P_1$ and $P_2$ using such prior information to make this particular approach applicable with reasonably strong test power. In addition to the direct access to the common eigenvectors for constructing \eqref{eqn:eigvP}, knowledge of common eigen-structures could also be that, when defining \eqref{eqn:polyP}, there is a reference square matrix which shares eigenvectors with the matrices to be tested.


\subsection{Summary of two-sample test}
The test methods developed in \autoref{sec:commute} and \autoref{sec:llrt} could be applied in different settings. For example, if the estimators are available with reasonable asymptotic normality, only the commutator-based test design would guarantee acceptable effectiveness; and if exact eigen-information is given with certainty, the LLR test could be a good choice. However, cases with such strong restrictions and adequate information could be rather rare in real applications. In the simulations and applications later, the commutator-based test is conducted.

For the commutator-based test \autoref{thm:comm.Est}, we see from \autoref{fig:comm} that with sample size increasing, the p-values of samples from null space ($\SNR = \infty$) tend to be uniformly distributed on the interval $[0,1]$, and the p-values of samples from alternative spaces start to concentrate in the interval $[0, 0.05)$. When the sample size $n$ exceeds a certain level, $250$ for instance, the test performs well with acceptable type I error and excellent type II error.

For the LLR test, we conduct (i) \autoref{thm:LLR} given either the exact $\mathcal{M}_2(\rho,d;d)$ for \eqref{eqn:polyP} or the exact eigenvectors $V$ for \eqref{eqn:eigvP}, and (ii) \autoref{thm:LLRest} with estimators $\mathcal{A}_2(\rho)$ plugged-in. By comparing the first and the third rows of \autoref{fig:LLR} with \autoref{fig:comm}, \autoref{thm:LLR} has better performance in terms of type II error, especially with small sample sizes. With the plugin version \autoref{thm:LLRest} (the second and the fourth rows), however, one frequently fails to distinguish the null hypothesis even under the ideal case when $\SNR = \infty$ or $\rho = 0$. It is compatible with our error analysis given in \autoref{sec:error}. To summarize, though with the well-behaved p-values of \autoref{thm:LLR} under $H_0$, such idea from \autoref{sec:llrt} may rarely be applicable unless the test is against some deterministic reference eigen-structure.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
    \centering
    \centerline{\input{Plots/PvalueLLR.tikz}}
    \caption{The histograms of p-values for the LLR test. The first / third row are results based on \autoref{thm:LLRest} given the exact polynomial basis \eqref{eqn:polyP} / eigenvector basis \eqref{eqn:eigvP}, while the second / fourth row are obtained by plugging estimated $\mathcal{A}_2(\rho)$ / optimized $\widehat{V}$ into \eqref{eqn:polyP} / \eqref{eqn:eigvP} for implementations of the first / third row.}
   \label{fig:LLR}
\end{figure}






\section{Extension to symmetric matrices}
\label{sec:ext}

As mentioned in the introduction, the analysis of common eigenvectors has many applications for symmetric matrices, for example, CPCA. The test methods introduced in this work can be implemented for symmetric matrices as well if we take additional care of the assumptions in \autoref{subsec:ass}.

Suppose the matrices $M_i$, $i=1,\dots,p$, and their respective estimators $A_i$, $i=1,\dots,p$, are symmetric matrices. Denote the function $\Vech: \mathbb{R}^{d \times d} \to \mathbb{R}^{d(d+1)/2}$ that converts a symmetric matrix $A$ into a vector stacking only distinct elements columnwise. Then, from estimations for symmetric $M_i$, the available consistency statements are of the form
$$
c(n) \Vech(A_i - M_i) \xrightarrow{\mathcal{D}} \mathcal{N}(0, \widetilde{\Sigma}_i)
$$
with positive semi-definite $\widetilde{\Sigma}_i \in \mathbb{R}^{d(d+1)/2 \times d(d+1)/2}$. There exists the duplication matrix $G_d \in \{0, 1\}^{d^2 \times d(d+1)/2}$ such that $G_d \Vech(A) = \Vector(A)$ for any symmetric $A \in \mathbb{R}^{d \times d}$; see \cite{magnus2019matrix} for more details on such operations. Hence, we can obtain exactly the same setup as \autoref{ass:normal}, since
$$
c(n) \Vector(A_i - M_i) = c(n) G_d \Vech(A_i - M_i) \xrightarrow{\mathcal{D}} \mathcal{N}(0, \Sigma_i)
$$
with $\Sigma_i = G_d \widetilde{\Sigma}_i G_d'$. It is then straightforward to implement the above test designs directly, except that we may require the input eigenvector matrix $V$ (or $\widehat{V}$) to be orthogonal. Such orthogonal matrices can be obtained referring to existing optimization schemes like FG-algorithm by \cite{Flury86}.

As we focus on the general asymmetric setting, our simulation study as well as the application section do not cover symmetric extensions.


\section{Proofs}\label{app:profs}

We provide here the proofs of most theoretical results except \Cref{thm:part.Est,,thm:multi_eig.Est,,thm:LLRest,,thm:comm.Est}. The proofs of those results can be found in \autoref{app:est} since they are based on a generic result.

\begin{proof}[Proof of \autoref{lem:ginverse}]
Under the conditions that given $\varepsilon > 0$ and $\varepsilon$ is not an eigenvalue of $\Sigma$, the mappings $\Sigma \mapsto \Sigma(\varepsilon)$, $\Sigma \mapsto \Sigma^+(\varepsilon)$ and $\Sigma \mapsto \rank(\Sigma; \varepsilon)$ are all at least locally continuous at $\Sigma$. Hence \eqref{eq:wald.cmt} follows by the continuous mapping theorem.
\end{proof}

\begin{proof}[Proof of \autoref{thm:comm}] \label{app:prop1}
Introduce the two vectors
$$
\bm{a}_0 = \big(\Vector(A_1)', \Vector(A_2)'\big)', ~ \bm{m}_0 = \big(\Vector(M_1)', \Vector(M_2)'\big)'.
$$
By \autoref{ass:normal},
\begin{equation} \label{eq:proof3eq1}
c(n) (\bm{a}_0 - \bm{m}_0) \xrightarrow{\mathcal{D}} \mathcal{N}(0, \Sigma_0)
\end{equation}
with $\Sigma_0 = \blkdiag(\Sigma_1, \Sigma_2)$. Recall that $[M_{1},M_{2} ]= M_{1}M_{2}-M_{2}M_{1}$ and define the function $g: \mathbb{R}^{2d^2} \to \mathbb{R}^{d^2}$ such that for $\bm{x} = \big(\bm{x}_1', \bm{x}_2'\big)'$ with $\bm{x}_1, \bm{x}_2 \in \mathbb{R}^{d^2}$,
$$
g(\bm{x}) = \Vector[\mat_d(\bm{x}_1), \mat_d(\bm{x}_2)].
$$
Under $H_0$, we know that $g(\bm{m}_0) = \bm{0}$ and $g(\bm{a}_0) = \bm{\eta}_n = \Vector[A_{1},A_{2} ]$, hence the asymptotic distribution of $\bm{\eta}_n$ can be derived via delta method.

Define
$$
\nabla_g := \nabla g(\bm{m}_0) = \begin{pmatrix}\Lambda(M_2)\\ \Lambda(M_1)\end{pmatrix}.
$$
Then, by delta method and \eqref{eq:proof3eq1},

$$
c(n) \big( g(\bm{a}_0) - g(\bm{m}_0) \big) \xrightarrow{\mathcal{D}} \mathcal{N}(0, \Sigma_\eta),
$$
where $\Sigma_\eta = \nabla_g' \Sigma_0 \nabla_g$. Then \eqref{eqn:gamma2} follows directly by the continuous mapping theorem.
\end{proof}

\begin{proof}[Proof of \autoref{thm:comm.Est}] 
This is a direct corollary from \autoref{thm:generalwald} in \autoref{app:est}.
\end{proof}


Before we prove \autoref{thm:ahat}, we introduce the following lemma.

\begin{lem}\label{lem:poly}
Suppose a matrix $C \in \R^{d \times d}$ has distinct real non-zero eigenvalues. Then any square matrix with the same eigenvectors as $C$ can be expressed by polynomials of $C$ with order less than $d$.
\end{lem}

\begin{proof}[Proof of \autoref{lem:poly}] \label{app:lempoly}
Assume $C = V D_C U$ where $V = (v_1, \dots, v_d)$ is the eigenvector matrix, $U = (u_1, \dots, u_d)' = V^{-1}$, and $D_C$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues. Then the matrices that also have $V$ as the eigenvector matrix form the following linear space:
$$
\mathfrak{S} = \{X: X = V (\sum_{i=1}^d \bm{a}_i E_i) U = \sum_{i=1}^d \bm{a}_i v_i u_i', ~ \bm{a} = (\bm{a}_1, \dots, \bm{a}_d) \in \mathbb{R}^{d}\},
$$
where the matrix $E_i \in \mathbb{R}^{d \times d}$ has only one non-zero element $1$ at the $i$th diagonal entry. Then the space $\mathfrak{S}$ has dimension $d$ and linear basis $\{v_i u_i'\}_{i=1}^d$.

Since $C$ has distinct real non-zero eigenvalues, according to Cayley-Hamilton theorem \citep[Theorem 2.4.3.2]{Horn}, the characteristic polynomial and the minimal polynomial of $C$ coincide and have degree $d$. Hence the matrices $\{C^0 = I_d, C^1, \dots, C^{d-1}\}$ form an independent basis and the polynomial space
$$
\mathfrak{S}_P = \{X: X = \sum_{i=1}^d \bm{b}_i C^{i-1}, ~ \bm{b} = (\bm{b}_1, \dots, \bm{b}_d) \in \mathbb{R}^{d}\}
$$
has dimension $d$ as well. In addition, since $\mathfrak{S}_P \subset \mathfrak{S}$ and both spaces have dimension $d$, it follows $\mathfrak{S}_P = \mathfrak{S}$.
\end{proof}

\begin{proof}[Proof of \autoref{thm:ahat}]

\autoref{lem:poly} and its proof immediately imply that under the null hypothesis $H_0$, there exist coordinate vectors $\bm{x}_1, \bm{x}_2 \in \R^d$, such that
\begin{equation}
    \Vector(M_1) = P_2 \bm{x}_1, ~ \Vector(M_2) = P_1 \bm{x}_2,\label{eq:x12}
\end{equation}
with matrix $P_i$, $i=1, 2$, defined by either \eqref{eqn:polyP} or \eqref{eqn:eigvP}.

Then the maximization problem in \eqref{eqn:Mest} can be transformed to optimizing with respect to free vectors $\bm{x}_1$ and $\bm{x}_2$ in $\mathbb{R}^d$. By setting the first-order derivative of function $L$ in \eqref{eqn:loglike} to zero, the estimators can then be solved explicitly as
\begin{align*}
    & \Vector(\widehat{A}_1) = P_2 (P_2' \Sigma_1^+ P_2)^+ P_2' \Sigma_1^+ \Vector(A_1),\\
    & \Vector(\widehat{A}_2) = P_1 (P_1' \Sigma_2^+ P_1)^+ P_1' \Sigma_2^+ \Vector(A_2).
\end{align*}
\end{proof}



\begin{proof}[Proof of \autoref{thm:LLR}]
Since $\Sigma_1$ is a positive semidefinite matrix, we can find the low-rank square root $\Sigma_1^{+/2} \in \mathbb{R}^{r \times d^2}$ of its general inverse, such that $\Sigma_1^+ = (\Sigma_1^{+/2})' \Sigma_1^{+/2}$, where $r$ is the rank of $\Sigma_1$. Then, by \autoref{ass:normal},
$$
c(n) \Sigma_1^{+/2} \Vector(A_1 - M_1) \xrightarrow{\mathcal{D}} \mathcal{N}(0, I_{r}).
$$
Set $G = I_{r} - \Sigma_1^{+/2} P_2 (P_2' \Sigma_1^+ P_2)^+ P_2' (\Sigma_1^{+/2})'$, then under $H_0$, the first summand of $\Gamma_2$ in \eqref{eqn:gamma1} is
\begin{align}
    &c^2(n) \Vector (A_1)' Q_{1,2} \Vector(A_1) \nonumber\\
    & = c^2(n) \Vector (A_1)' \Big(\Sigma_1^+ - \Sigma_1^+ P_2 (P_2' \Sigma_1^+ P_2)^+ P_2' \Sigma_1^+\Big) \Vector(A_1) \nonumber\\
    & = c^2(n) \big(\Sigma_1^{+/2} \Vector(A_1)\big)' G \big(\Sigma_1^{+/2} \Vector(A_1)\big) \notag \\
    & = c^2(n) \big(\Sigma_1^{+/2} \Vector(A_1 - M_1)\big)' G \big(\Sigma_1^{+/2} \Vector(A_1 - M_1)\big).\label{eqn:addzeros}
\end{align}
The equality in \eqref{eqn:addzeros} uses the fact that $G \Sigma_1^{+/2} \Vector(M_1) = G \Sigma_1^{+/2} P_2 \bm{x}_1 = 0$. In addition, the matrices $G$ and $I_{r} - G$ are both projection matrices, and $I_{r} - G$ projects matrices onto the column space of $\Sigma_1^{+/2} P_2$, hence $\rank(I_{r} - G) = \rank(P_2' \Sigma_1^+ P_2)$ and $\rank(G) = r - \rank(P_2' \Sigma_1^+ P_2) = r_{1,2}$ with $r_{1,2}$ defined in \autoref{thm:LLR}. Then the first summand in \eqref{eqn:gamma1} satisfies
$$
c^2(n) \big(\Sigma_1^{+/2} \Vector(A_1 - M_1)\big)' G \big(\Sigma_1^{+/2} \Vector(A_1 - M_1)\big) \xrightarrow{\mathcal{D}} \chi^2(r_{1,2}).
$$
Similarly, the second summand in $\Gamma_2$, $c^2(n) \Vector (A_2)' Q_{2,1} \Vector(A_2)$, converges to a chi-square distribution with $r_{2,1}$ degrees of freedom, and since the two summands are independent, the result \eqref{eqn:projTest} follows.
\end{proof}


\begin{proof}[Proof of \autoref{prop:alt.behave}]
Denote
\begin{equation}\label{eqn:sigmaHalf}
    \widehat{\Sigma}_1^+(\varepsilon) = (\widehat{\Sigma}_{1,\varepsilon}^{+/2})' \widehat{\Sigma}_{1, \varepsilon}^{+/2}, \hspace{0.3cm} \text{ with } \hspace{0.3cm} \widehat{\Sigma}_{1,\varepsilon}^{+/2} \in \mathbb{R}^{\widehat{r} \times d^2}, ~ \widehat{r}(\varepsilon) = \rank(\widehat{\Sigma}_1; \varepsilon)
\end{equation}
and
$$
G[\varepsilon] = I_{\widehat{r}(\varepsilon)} - \widehat{\Sigma}_{1,\varepsilon}^{+/2} P_2 \big(P_2' \widehat{\Sigma}_1^+(\varepsilon) P_2\big)^+ P_2' (\widehat{\Sigma}_{1,\varepsilon}^{+/2})'.
$$
Then, the first summand of $\Gamma_2^\#(\varepsilon)$ can be written as
\begin{equation} \label{eqn:alt.zeros}
\begin{aligned}
    &c^2(n) \big(\widehat{\Sigma}_{1,\varepsilon}^{+/2} \Vector(A_1)\big)' G[\varepsilon] \big(\widehat{\Sigma}_{1,\varepsilon}^{+/2} \Vector(A_1)\big)\\
    &=c^2(n) \big(\widehat{\Sigma}_{1,\varepsilon}^{+/2} \Vector(A_1 - M_1)\big)' G[\varepsilon] \big(\widehat{\Sigma}_{1,\varepsilon}^{+/2} \Vector(A_1 - M_1)\big)\\
    & \hspace{1cm}
    + 2 c^2(n) \big(\widehat{\Sigma}_{1,\varepsilon}^{+/2} \Vector(A_1 - M_1)\big)' G[\varepsilon] \widehat{\Sigma}_{1,\varepsilon}^{+/2} \bm{m}_1\\
    & \hspace{2cm}+ c^2(n) \bm{m}_1' (\widehat{\Sigma}_{1,\varepsilon}^{+/2})' G[\varepsilon]
     \widehat{\Sigma}_{1,\varepsilon}^{+/2} \bm{m}_1.
\end{aligned}
\end{equation}
The equality in \eqref{eqn:alt.zeros} uses the fact that
$$
G[\varepsilon] \widehat{\Sigma}_{1,\varepsilon}^{+/2} \Vector(M_1) = G[\varepsilon] \widehat{\Sigma}_{1,\varepsilon}^{+/2} \bm{m}_1.
$$
In addition, it can be verified that $\bm{m}_1' \widehat{\Sigma}_1^+(\varepsilon) P_2 = 0$, then
$$
\bm{m}_1' (\widehat{\Sigma}_{1,\varepsilon}^{+/2})' G[\varepsilon] \widehat{\Sigma}_{1,\varepsilon}^{+/2} \bm{m}_1 = \bm{m}_1' \widehat{\Sigma}_1^+(\varepsilon) \bm{m}_1 > 0,
$$
since $\widehat{\Sigma}_1^+(\varepsilon) \bm{m}_1 \neq 0$. Hence, the deterministic leading term in \eqref{eqn:alt.zeros}, $c^2(n) \bm{m}_1' \widehat{\Sigma}_1^+(\varepsilon) \bm{m}_1$, goes to infinity as $c(n)$ goes to infinity. Similar arguments can be used for the second summand of $\Gamma_2^\#(\varepsilon)$ and the proposition is proved.
\end{proof}



%\begin{equation}
%    \Delta_\varepsilon := 
%    c^2(n) \Vector (A_1)' (\widehat{Q}_{1,2}[\varepsilon] - \widehat{\mathcal{Q}}_{1,2}[\varepsilon]) \Vector(A_1) \label{eqn:deltaeps}
%\end{equation}
%with $\widehat{Q}_{1,2}[\varepsilon]$ as in \eqref{eq:Qkl} and $\widehat{\mathcal{Q}}_{1,2}[\varepsilon]$ is defined by replacing the matrices $P_{\ell}$ in \eqref{eq:Qkl} by their sample counterparts $\widehat{P}_{\ell}$ such that
%\begin{equation} \label{eqn:mathcalQ}
%\widehat{\mathcal{Q}}_{k,\ell}[\varepsilon] := \widehat{\Sigma}_k^+(\varepsilon) - \widehat{\Sigma}_k^+(\varepsilon) \widehat{P}_\ell \big(\widehat{P}_\ell' \widehat{\Sigma}_k^+(\varepsilon) \widehat{P}_\ell\big)^+ \widehat{P}_\ell' \widehat{\Sigma}_k^+(\varepsilon)
%\end{equation}

\begin{proof}[Proof of \autoref{lem:asyerror}] \label{app:error}
%We first approximate $\Delta_\varepsilon$ 

For $\Delta_\varepsilon$ in \eqref{eqn:deltaeps}, use the notation of $\widehat{\Sigma}_{1,\varepsilon}^{+/2}$ introduced in \eqref{eqn:sigmaHalf} to simplify 
\begin{equation} %\label{eqn:mathcalQ}
\widehat{Q}_{1,2}[\varepsilon] - \widehat{\mathcal{Q}}_{1,2}[\varepsilon]
= \big(\widehat{\Sigma}_{1,\varepsilon}^{+/2}\big)' (\Delta \widehat{Q}_\varepsilon) \widehat{\Sigma}_{1,\varepsilon}^{+/2},
\end{equation}
where
$$
\Delta \widehat{Q}_\varepsilon = \widehat{Q}_\varepsilon(P_2) - \widehat{Q}_\varepsilon(\widehat{P}_2),
$$
with
$\widehat{Q}_\varepsilon: \mathbb{R}^{d^2 \times d} \to \mathbb{R}^{\widehat{r}(\varepsilon) \times \widehat{r}(\varepsilon)}$ a matrix-valued projection function given by
$$
\widehat{Q}_\varepsilon(P) := \widehat{\Sigma}_{1,\varepsilon}^{+/2} P \big(P' \widehat{\Sigma}_1^+(\varepsilon) P\big)^+ P' \big(\widehat{\Sigma}_{1,\varepsilon}^{+/2}\big)'.
$$
Then, the first-order-perturbation approximation can be calculated as
\begin{equation}\label{eqn:delta1}
\begin{aligned}
    \Delta_\varepsilon &= 
    c^2(n) \Vector (A_1)' (\widehat{Q}_{1,2}[\varepsilon] - \widehat{\mathcal{Q}}_{1,2}[\varepsilon]) \Vector(A_1) \\
    &=
    c^2(n) \Vector(A_1)' \big(\widehat{\Sigma}_1^{+/2}(\varepsilon)\big)' (\Delta \widehat{Q}_\varepsilon) \widehat{\Sigma}_1^{+/2}(\varepsilon) \Vector(A_1)\\
    &=
    2 c^2(n) \Vector(A_1)' \widehat{\Sigma}_1^{+}(\varepsilon) P_2 \big(P_2' \widehat{\Sigma}_1^+(\varepsilon) P_2\big)^+ (\Delta P_2)' (\widehat{\Sigma}_{1,\varepsilon}^{+/2} )' \times \\
    & \hspace{1cm} \big(I_{\widehat{r}} - \widehat{Q}_\varepsilon(P_2)\big) \widehat{\Sigma}_{1,\varepsilon}^{+/2} \Vector(A_1 - M_1)
    + o_{\mathcal{P}}\big(c^2(n) \Delta P_2\big)\\
    &= 2 c^2(n) \Vector(A_1)' \Sigma_1^{+} P_2 \big(P_2' \Sigma_1^+ P_2\big)^+ (\Delta P_2)' (\Sigma_{1}^{+/2} )' \times \\
    & \hspace{1cm} G \Sigma_1^{+/2} \Vector(A_1 - M_1)
    + o_{\mathcal{P}}\big(c^2(n) \Delta P_2\big)
\end{aligned}
\end{equation}
where $\Delta P_2 = \widehat{P}_2 - P_2$. Here in the last equality, we use the assumption that $\Sigma_1(\varepsilon) = \Sigma_1$, hence $\widehat{\Sigma}_1^+(\varepsilon) = \Sigma_1^+ + o_{\mathcal{P}}(1)$, $\widehat{\Sigma}_{1,\varepsilon}^{+/2} = \Sigma_1^{+/2} + o_{\mathcal{P}}(1)$, and $I_{\widehat{r}} - \widehat{Q}_\varepsilon(P_2) = G + o_{\mathcal{P}}(1)$.

From now on we omit the $o_{\mathcal{P}}\big(c^2(n) \Delta P_2\big)$ term of $\Delta_\varepsilon$ in \eqref{eqn:delta1} for the following proof since we are only interested in its stochastic limit.

Then, assume $G = U' U$ where $U \in \mathbb{R}^{r_{1,2} \times r}$, $U U' = I_{r_{1,2}}$, define
$$
\bm{w} = (\bm{y}', \bm{z}')', \hspace{0.45cm} \bm{y} := U \Sigma_1^{+/2} \Delta P_2 \big(P_2' \Sigma_1^+ P_2\big)^+ P_2' \Sigma_1^{+} \Vector(A_1), \hspace{0.45cm} \bm{z} := U \Sigma_1^{+/2} \Vector(A_1 - M_1).
$$
%with
%$$
%\begin{aligned}
%    \bm{y} := U \Sigma_1^{+/2} \Delta P_2 \big(P_2' \Sigma_1^+ P_2\big)^+ P_2' \Sigma_1^{+} \Vector(A_1), \hspace{0.3cm}
%    \bm{z} := U \Sigma_1^{+/2} \Vector(A_1 - M_1).
%\end{aligned}
%$$

By \autoref{ass:normal}, $A_1$ converges in probability to the true matrix $M_1$, and $\widehat{P}_2$ is a consistent estimator of $P_2$ with rate $c(n)$, then according to delta method and Slutsky's theorem, we can define the matrix $\widecheck{\Sigma} = \widecheck{\Sigma}(M_1, M_2, \Sigma_1, \Sigma_2)$ in \autoref{lem:asyerror} to satisfy
$$
c(n) \bm{y} = c(n) U \Sigma_1^{+/2} \Delta P_2 \big(P_2' \Sigma_1^+ P_2\big)^+ P_2' \Sigma_1^{+} \Vector(A_1) \xrightarrow{\mathcal{D}} \mathcal{N}(0, \widecheck{\Sigma}).
$$
In addition, we have
$$
c(n) \bm{z} = c(n) U \Sigma_1^{+/2} \mbox{vec}(A_1 - M_1) \xrightarrow{\mathcal{D}} \mathcal{N}(0, I_{r_{1,2}}).
$$
And note that due to the independence between $A_1$ and $\Delta P_2$, the following expectation can be decomposed to
\begin{equation}\label{eqn:expProd}
    \mathbb{E}\Big[\big(c(n) \bm{z}\big) \big(c(n) \bm{y}\big)'\Big] = U \Sigma_1^{+/2} \mathbb{E} \bigg[ c(n) f(A_1) \bigg] \mathbb{E}\big[c(n) \Delta P_2' \big] (\Sigma_1^{+/2})' U',
\end{equation}

where $f(A_1) = \mbox{vec}(A_1 - M_1) \big(\Vector(A_1)\big)' \Sigma_1^{+} P_2 \big(P_2' \Sigma_1^+ P_2\big)^+$ is a matrix whose only randomness is from $A_1$. And since $c(n) \Delta P_2'$ has asymptotically zero mean, the expectation \eqref{eqn:expProd} is asymptotically zero, which indicates that $c(n) \bm{y}$ and $c(n) \bm{z}$ are asymptotically uncorrelated. Hence we have
$$
c(n) \bm{w} = c(n) \begin{pmatrix} \bm{y} \\ \bm{z} \end{pmatrix} \xrightarrow{\mathcal{D}} \mathcal{N}\Bigg(0, ~ \begin{pmatrix} \widecheck{\Sigma} & 0 \\ 0 & I_{r_{1,2}} \end{pmatrix}\Bigg),
$$
and consequently by the continuous mapping theorem,
$$
\Delta_\varepsilon = 2 c^2(n) \bm{w}' \begin{pmatrix} 0 & I_{r_{1,2}} \\ I_{r_{1,2}} & 0 \end{pmatrix} \bm{w} \xrightarrow{\mathcal{D}} 
Z, \hspace{0.12cm} \text{ with } \hspace{0.12cm} Z= 
\sum_{i = 1}^{r_{1,2}} 2\sqrt{\lambda_i} (\nu_{i,1} - \nu_{i,2}),
$$
where $\lambda_i$ are the eigenvalues of $\widecheck{\Sigma}$ and $\nu_{i,j} \stackrel{i.i.d.}{\sim} \chi^2(1)$ for $i \in \{1, \dots, r_{1,2}\}$, and $j = 1,2$. In addition, note that the variance of a $\chi^2(1)$ distributed random variable is 2. For this reason,
$$
\var(Z) = \sum_{i=1}^{r_{1,2}} 4 \lambda_i \big(\var(\nu_{i,1}) + \var(\nu_{i,2})\big) = 16 \sum_{i=1}^{r_{1,2}} \lambda_i = 16 \tr(\widecheck{\Sigma}).
$$
\end{proof}
\section{Generalized Wald test}\label{app:est}

In this section we provide a generic result to prove \Cref{thm:part.Est,,thm:multi_eig.Est,,thm:LLRest,,thm:comm.Est}. In fact, \autoref{thm:generalwald} below combined together with the asymptotic normality results proved in \Cref{thm:comm,,thm:LLR,,thm:multi_eig,,thm:part} gives \Cref{thm:part.Est,,thm:multi_eig.Est,,thm:LLRest,,thm:comm.Est}.

Consider an estimated vector $\widehat{\bm{v}}$ that depends on the sample size $n$ and satisfies the asymptotic normality statement 
\begin{equation}\label{eq:wald.asym}
    c(n) (\widehat{\bm{v}} - \bm{v}) \xrightarrow{\mathcal{D}} \mathcal{N}(0, \Sigma)
\end{equation}
with deterministic $\bm{v}$ and positive semidefinite limiting covariance matrix $\Sigma \in \mathbb{R}^{\tau \times \tau}$. Then, a Wald-type test statistic can be defined as
\begin{equation}\label{eq:Wald.eg}
    \Gamma = c^2(n) (\widehat{\bm{v}} - \bm{v})' \Sigma^+ (\widehat{\bm{v}} - \bm{v}) \xrightarrow{\mathcal{D}} \chi^2(\rank(\Sigma)).
\end{equation}
If $\Sigma$ is singular, the continuous mapping theorem is no longer applicable to justify replacing $\Sigma$ by a consistent estimator $\widehat{\Sigma}$ in \eqref{eq:Wald.eg}. In order to achieve a convergence result for $\Gamma$ in \eqref{eq:Wald.eg} based on an estimator for $\Sigma$, we introduce the following theorem.

\begin{thm}\label{thm:generalwald}
Assume \eqref{eq:wald.asym} holds and $\widehat{\Sigma}$ is a consistent estimator of $\Sigma$, threshold $\varepsilon > 0$ is not an eigenvalue of $\Sigma$. Then, alternative to \eqref{eq:Wald.eg}, we propose statistics with truncated SVD introduced in \autoref{subsec:Cov},
\begin{equation}\label{eq:wald.new.exact}
    \Gamma(\varepsilon) = c^2(n) (\widehat{\bm{v}} - \bm{v})' \Sigma^+(\varepsilon) (\widehat{\bm{v}} - \bm{v}) \xrightarrow{\mathcal{D}} \chi^2\big(\rank(\Sigma; \varepsilon)\big),
\end{equation}
and
\begin{equation}\label{eq:wald.new.est}
    \Gamma^\#(\varepsilon) = c^2(n) (\widehat{\bm{v}} - \bm{v})' \widehat{\Sigma}^+(\varepsilon) (\widehat{\bm{v}} - \bm{v}) \xrightarrow{\mathcal{D}} \chi^2\big(\rank(\widehat{\Sigma}; \varepsilon)\big).
\end{equation}
\end{thm}

Indeed if the threshold $\varepsilon$ is less than the smallest non-zero eigenvalue of $\Sigma$, especially when at the extreme case $\varepsilon = 0$, the limiting distribution in \eqref{eq:wald.new.exact} is exactly the same as \eqref{eq:Wald.eg}.

\begin{proof}[Proof of \autoref{thm:generalwald}]
Assume SVD gives $\Sigma = Z \Pi Z'$ with orthogonal $Z$, then by definition of the truncated SVD,
$$
\Sigma \times \Sigma^+(\varepsilon) = \Sigma^+(\varepsilon) \times \Sigma = Z \begin{pmatrix} I_{\rank(\Sigma; \varepsilon)} & 0 \\ 0 & 0 \end{pmatrix} Z'
$$
is an idempotent matrix with rank $\rank(\Sigma; \varepsilon)$.
Then, \eqref{eq:wald.new.exact} follows immediately by the continuous mapping theorem. According to \eqref{eq:wald.cmt} in \autoref{lem:ginverse}, we can find consistent estimators for the generalized inverse as well as for the rank of $\Sigma$. \autoref{lem:ginverse}, \eqref{eq:wald.new.exact} and the continuous mapping theorem prove \eqref{eq:wald.new.est}.
\end{proof}

\end{appendix}


\small
%\bibliographystyle{Chicago}
\bibliographystyle{plainnat}
% \bibliographystyle{plain}
\bibliography{main}







\end{document}