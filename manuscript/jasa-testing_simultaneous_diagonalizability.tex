\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%


\RequirePackage[OT1]{fontenc}
\usepackage{amsthm,amsfonts,amssymb,mathtools,tikz}
\usepackage{graphicx,bm,environ}
\usepackage{mathabx}
\usepackage{float}
\usepackage[boxed, ruled, lined, linesnumbered]{algorithm2e}
%\setlength{\bibsep}{-0.5pt}
\usepackage{xr-hyper}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage{setspace}

% provide arXiv number if available:
%\arxiv{2101.07776}

% put your definitions there:
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newtheorem{ass}{Assumption}
\newcommand{\assautorefname}{Assumption}
\newtheorem{thm}{Theorem}
\numberwithin{thm}{section}
\newcommand{\thmautorefname}{Theorem}
\newtheorem{defn}{Definition}
\numberwithin{defn}{section}
\newcommand{\defnautorefname}{Definition}
\newtheorem{lem}{Lemma}
\numberwithin{lem}{section}
\newcommand{\lemautorefname}{Lemma}
\newtheorem{prop}{Proposition}
\numberwithin{prop}{section}
\newcommand{\propautorefname}{Proposition}
\newtheorem{cor}{Corollary}
\numberwithin{cor}{section}
\newcommand{\corautorefname}{Corollary}
\newtheorem{rem}{Remark}
\numberwithin{rem}{section}
\newcommand{\remautorefname}{Remark}
\renewcommand{\sectionautorefname}{Section}
\let\subsectionautorefname\sectionautorefname 	\let\subsubsectionautorefname\sectionautorefname
\renewcommand{\algorithmautorefname}{Algorithm}

\Crefname{ass}{Assumption}{Assumptions}
\Crefname{prop}{Proposition}{Propositions}
\Crefname{section}{Section}{Sections}
\Crefname{appendix}{Appendix}{Appendices}


\DeclareMathOperator*{\argmin}{arg min}
\DeclareMathOperator*{\argmax}{arg max}
\DeclareMathOperator{\Vector}{vec}
\DeclareMathOperator{\Vech}{vech}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\offvec}{offvec}
\DeclareMathOperator{\blkdiag}{blkdiag}
\DeclareMathOperator{\SNR}{SNR}
\DeclareMathOperator{\mat}{mat}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\diag}{diag}
\newcommand{\rank}{\operatorname{rk}} 
%\endlocaldefs

%\addbibresource{main.bib}


% \makeatletter
% \newcommand*{\addFileDependency}[1]{% argument=file name and extension
%   \typeout{(#1)}
%   \@addtofilelist{#1}
%   \IfFileExists{#1}{}{\typeout{No file #1.}}
% }
% \makeatother

% \newcommand*{\myexternaldocuments}[1]{%
%     \externaldocument{#1}%
%     \addFileDependency{#1.tex}%
%     \addFileDependency{#1.aux}%
% }

% \myexternaldocuments{supplement}

\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


  \title{\LARGE\bf Testing Simultaneous Diagonalizability}
  \author{Yuchen Xu\thanks{Corresponding author: Yuchen Xu, Email address: \href{mailto:yx439@cornell.edu}{yx439@cornell.edu}}
    \and
    Marie-Christine D\"uker\thanks{Email address: \href{mailto:md2224@cornell.edu}{md2224@cornell.edu}}
    \and
    David S. Matteson\thanks{Email address: \href{mailto:matteson@cornell.edu}{matteson@cornell.edu}}
    \hspace{.7cm}\\
    Department of Statistics and Data Science, Cornell University}
  \maketitle





\bigskip
\begin{abstract}
This paper proposes novel methods to test for simultaneous diagonalization of possibly asymmetric matrices. Motivated by various applications, a two-sample test as well as a generalization for multiple matrices are proposed. A partial version of the test is also studied to check whether a partial set of eigenvectors is shared across samples. Additionally, a novel algorithm for the considered testing methods is introduced. Simulation studies demonstrate favorable performance for all designs. 
Finally, the theoretical results are utilized to decouple multiple vector autoregression models into univariate time series, and to test for the same stationary distribution in recurrent Markov chains.
These applications are demonstrated using macroeconomic indices of 8 countries and streamflow data, respectively.
\end{abstract}

\noindent%
{\it Keywords:}  
common eigenvectors, joint diagonalization, partially common eigenvectors, likelihood ratio test, vector autoregression, Markov chain, Wald test, dimension reduction.
\vfill




\newpage
\spacingset{1.9}

\section{Introduction}\label{sec:intro}
\vspace{-0.3cm}
Understanding the eigenvectors and the eigenspace of matrix-valued objects is known to be of fundamental interest in various disciplines including statistics, machine learning, and computer science.
Knowledge about the eigenvectors and the eigenspace is particularly valuable in principal component analysis (PCA) 
%\citep{pearson1901liii,jolliffe1986principal,nadler2008finite,cai2013sparse,koltchinskii2017new}
\citep{nadler2008finite,cai2013sparse,koltchinskii2017new}
, covariance matrix estimation \citep{fan2013large,fan2015estimation,fan2018eigenvector}, spectral clustering \citep{von2007tutorial,rohe2011spectral,lei2015}, and network or graph theory \citep{tang2018,paul2020spectral}.
Often times it provides information for dimension reduction and clustering procedures.

%Equally interesting but much less explored are the eigenvectors and the eigenspace of possibly asymmetric matrices and their potential applications.
This paper develops statistical tests and algorithms to check whether a set of square matrices can be diagonalized simultaneously. We are particularly interested in asymmetric square matrices with more general and flexible structural assumptions compared to symmetric ones like covariance matrices. Our work proceed from two-sample tests to multi-sample tests, and finally extends into partial cases where only a subset of eigenvectors is of interest.
Besides providing the theoretical foundation and introducing practical algorithms, we motivate the usefulness of our results in several examples.

%\subsection{Problem setup} \label{sec:setup}
Our setting is as follows.
Suppose we have a sequence of deterministic matrices $\{M_i\}_{i = 1}^p \subset \mathbb{R}^{d \times d}$. Then the hypothesis testing problem we are interested in can be expressed as: the null hypothesis is $H_0$: $\{M_i\}_{i = 1}^p$ can be jointly diagonalized, or equivalently, 
%we can express the null hypothesis in mathematical form as:
\begin{multline}\label{hyp:all}
    H_0: ~ \exists V \in \mathbb{R}^{d \times d}, ~ s.t. ~ M_i V = V D_i,\
    \forall i = 1, \dots, p, ~ D_i \in \mathcal{M}_{\diag}(d),
    %\mathbb{R}^{d \times d} \mbox{ are diagonal matrices.}
\end{multline}
where $\mathcal{M}_{\diag}(d)$ denotes the set of diagonal matrices in $\mathbb{R}^{d \times d}$.
%The alternative is $H_1$: otherwise.
A modification of the problem is to discuss whether a set of matrices shares a partial set of eigenvectors. The null hypothesis is then expressed as $H_0^*$: $\{M_i\}_{i = 1}^p$ share $k$ left eigenvectors ($k<d$), or equivalently, 
\begin{multline} \label{hyp:part}
    H_0^*: ~ \exists V = (v_1, \dots, v_k) \in \mathbb{R}^{d \times k} \text{ full rank}, s.t. ~ M_i V = V D_i, ~ D_i \in \mathcal{M}_{\diag}(k).
    %\\
    %~ s.t. ~ M_i V = V D_i, ~ D_i \in \mathbb{R}^{k \times k} \mbox{ are diagonal matrices.}\label{hyp:part}
\end{multline}

%Similar questions under much less flexible assumptions on the pool of matrices were considered before.
In a series of contributions \cite{Flury84,FluryAsymptotic,Flury86} introduced common principal component analysis (CPCA) that deals with the test and calculation of simultaneous factorization among different samples of positive definite symmetric matrices. 
%\cite{Flury86} proposed the so-called FG-algorithm. 
Under the same assumptions, \cite{schott} developed the terminology partial CPCA (PCPCA) with verified test methods for partially identical eigenvectors. 
In contrast, we do not need to impose any structural assumptions on the pool of matrices like symmetry or positive semidefinitenss which are naturally provided by considering covariance matrices.

Related to simultaneous diagonalization, previous studies mainly focused on testing whether the eigenvectors or eigenspaces of the population covariance matrix are equal to some given ones; see \cite{tyler,koltchinskii2017new,silin2018bayesian,naumov2019bootstrap,silin2020hypothesis}. \cite{Schwartz} studied some related statistical tests about eigenvalues and eigenvectors of Gaussian random symmetric matrices with some pre-fixed algebraic restrictions.
Especially, as stated in \cite{Schwartz}, the test for equality of eigenvectors with unknown eigenvalues between two sets of samples is rather difficult since no closed forms of estimations are available.

From a computational perspective, optimization routines for symmetric matrices were proposed by \cite{Fuji, Ghazi, Gira}. For general asymmetric matrices, some previous ideas, like '\textit{sh-rt}' by \cite{shrt}, '\textit{JUST}' by \cite{just}, '\textit{JDTM}' by \cite{jdtm}, and '\textit{(W)JDTE}' by \cite{andre} are shown to be numerically effective and ready for implementation. \cite{colombo, tensor} focused on the joint Schur-decomposition and provided theoretical properties of their proposed algorithms. Since joint Schur-decomposability fails to be a sufficient condition for simultaneous diagonalization, our work expands those ideas and provides an algorithm which estimates partially common eigenvectors across samples. 

Possibly asymmetric matrix-valued statistics are broadly utilized in estimating the mean of random matrices, the adjacency matrices of weighted directed graphs, the coefficient matrices in linear regressions, factor models and vector autoregression (VAR) models, and transition probability matrices. However, most of the analysis has focused on studying the eigenvalues of those statistics. Analyses of eigenvalues include reduced rank estimation \citep{robin:2000tests, Kleibergen2006:Generalized, donald:2007rank}, testing for cointegration %\citep{Engle1987:Co,johansen91:Est,Maddala1999:Unit,vogelsang2001unit,zhang2019identifying} 
\citep{Engle1987:Co,johansen91:Est,vogelsang2001unit,zhang2019identifying}
and the eigenvalues of adjacency matrices \citep{restrepo2007approximating,paul2020spectral}. In contrast, our applications (see \autoref{subs:applic} below) give a new perspective on the usefulness of studying the eigenvectors in various models.

The literature review shows that existing work is based on covariance matrices which are surely diagonalizable with orthogonal eigenvectors. 
The eigenstructure problem lacks analysis in some more general cases like asymmetric matrices in particular. The breakthrough point of our work is to design and validate efficient diagonalization test methods for possibly asymmetric matrices.
Due to non-linearity of eigenproperties and the lack of closed eigensolutions,
our investigation about the random eigenstructures with less restricted conditions is algebraically difficult, and our exploration is novel.

\vspace{-0.5cm}
\subsection{Applications} \label{subs:applic}
\vspace{-0.3cm}
From a statistical perspective, joint diagonalizability provides valuable information.
Suppose one fails to reject the null hypothesis of common eigenvectors, then, it is reasonable to only analyze the eigenvalues, which reduces the problem's complexity significantly. 
We will illustrate the usefulness of our results with two relevant examples, namely the coefficient matrices in VAR models and the transition matrices of Markov chains.

The coefficient matrices in VAR models appear to be general matrices without restrictions like symmetry. 
%Multi-lagged VAR models assume close relationships between current and lagged status of data, which is very likely to impose similar intrinsic algebraic structures on the coefficient matrices. 
For the same multivariate time series of comparable objects, simple VAR models of order one may share common components in regression which can be verified by a joint eigendecomposition of coefficient matrices. With successful verification of simultaneous diagonalizability, one can decouple multivariate time series into multiple univariate ones and conduct comparison conveniently; see \Cref{se:VAR} for more details.

Another motivating example for our tests are the transition matrices of Markov chains which are usually asymmetric. Furthermore, the leading left eigenvector of a transition matrix corresponds to eigenvalue one and represents the stationary distribution of the chain. Testing the equality of the leading eigenvectors of the transition matrices from multiple Markov chains gives information whether these chains share similar properties though differing in their transition dynamics. For instance, the Markov chains of the same object but with different time resolutions might exhibit a common stationary distribution; see also \autoref{se:Markov} for more details on this application.

%The adjacency matrices of weighted directed graphs encode the similar information. Furthermore, some other applications that utilize asymmetric matrix-valued functionals include regression, factor models, and autocovariance estimation, but the meaning of the eigenvectors requires further exploration.


\vspace{-0.5cm}
\subsection{Organization}
\vspace{-0.3cm}
The rest of this paper is organized as follows. \autoref{sec:prel} establishes notation and gives some preliminary results.
Our main work starts in \autoref{sec:commute} from a two-sample test. In addition to simultaneously conducting two-sample tests pairwise for multi-sample cases, we design our test method based on the pooled estimator of common eigenvectors; see \autoref{sec:multi}. In \autoref{sec:part} we further extend our results to a partial version with a novel algorithm to estimate the subset of eigenvectors that is shared across samples. In \Cref{sec:simu,,sec:app} we conduct a simulation study and experiment with some real data examples, respectively. The supplementary material provides some empirical results complementary to the numerical analysis presented in the main paper; see {\color{cyan} Appendix A}.
Furthermore, an alternative approach for the two sample test is presented in {\color{cyan} Appendix B}.
%Furthermore it discusses how the Data for our simulations study were generated; see \Cref{subse:DGP}.
We also briefly show the compatibility of our test methods with the symmetric setting in {\color{cyan} Appendix C}. Finally, the proofs of the theoretical results in \Cref{sec:commute,sec:multi,sec:part} and {\color{cyan} Appendix B} can be found in the supplementary material in {\color{cyan} Appendices D} and {\color{cyan} E}.



\vspace{-0.5cm}
\section{Preliminaries} \label{sec:prel}
%In this section, we first introduce the notation used throughout the paper (\autoref{subsec:not}) and then give the required assumptions for future proof of the asymptotic results for our proposed test statistics (\autoref{subsec:ass}). We conclude with a workaround to deal with possibly low-rank covariance matrices in our test statistics (\autoref{subsec:Cov}).
\vspace{-0.3cm}
\subsection{Notation} \label{subsec:not}
\vspace{-0.5cm}
Throughout this paper, $p$ is the number of matrices to be tested, $n$ denotes the sample size for estimation, and $d$ is the dimension of the square matrices. Notation $\xrightarrow{\mathcal{D}}$ represents convergence in distribution, $\xrightarrow{\mathcal{P}}$ convergence in probability, and $\stackrel{\mathcal{D}}{\approx}$ an approximation of random distributions. The operator $\otimes$ denotes the Kronecker product between two matrices.
For a matrix $A$, the operator $\rank(A)$ denotes the rank of $A$, $\Vector(A)$ transforms $A$ into a vector form by stacking all its columns, and $A^+$ is the Moore-Penrose general inverse of $A$. For a square matrix $A$ of dimension $d$, we write $\tr(A)$ for the trace function of $A$ and the operator $\mat_d(\cdot)$ is the inverse of $\Vector(\cdot)$ such that $\mat_d\big(\Vector(A)\big) = A$.
%For a square matrix $A \in \mathbb{R}^{d \times d}$, the operator $\tr(A)$ denotes the trace function of $A$, the operator $\rank(A)$ denotes the rank of $A$, the operator $\Vector(A)$ transforms $A$ into a vector form by stacking all its columns, the operator $\mat_d(\cdot)$ is the inverse of $\Vector(\cdot)$ such that $\mat_d\big(\Vector(A)\big) = A$, and $A^+$ is the Moore-Penrose general inverse of $A$.
The matrix $I_d$ represents the $d$-dimensional identity matrix, and the function $\blkdiag(\{X_i\}_{i=1}^p)$ returns a block-diagonal matrix with the sub-matrices on the diagonal to be the input list of matrices $\{X_i\}_{i=1}^p$. We write $\mathcal{N}(\mu, \Sigma)$ for the multivariate normal distribution with mean vector $\mu$ and covariance matrix $\Sigma$, $\chi^2(k)$ for the chi-squared distribution with $k$ degrees of freedom, and $\gamma(\alpha, \beta)$ for the Gamma distribution with shape parameter $\alpha$ and rate parameter $\beta$.

\vspace{-0.3cm}
\subsection{Assumptions} \label{subsec:ass}
\vspace{-0.3cm}
In this section, we give the required assumptions for future proof of the asymptotic results for our proposed test statistics. 
%and some remarks to comment on those assumptions.

\begin{ass}\label{ass:normal}
The deterministic matrices $\{M_i\}_{i = 1}^p$ can be estimated by mutually independent estimators $\{A_{i,n}\}_{i = 1}^p$ from $n$ samples, satisfying,
\begin{equation}\label{eq:normal}
    c(n)\Vector(A_{i,n} - M_i) \xrightarrow{\mathcal{D}} \mathcal{N}(0, \Sigma_i) \hspace{0.2cm} \text{ for } \hspace{0.2cm} i=1,\dots,p,
\end{equation}
with $c(n) \to \infty$ as $n \to \infty$.
\end{ass}

\begin{ass}\label{ass:covConsistent}
The limiting covariance matrices $\Sigma_i$ in \eqref{eq:normal} can be estimated consistently by $\widehat{\Sigma}_{i,n}$, that is,
\begin{equation}\label{eq:sigma.rate}
    \widehat{\Sigma}_{i,n} - \Sigma_i = O_{\mathcal{P}}\big(\sigma(n)\big)
     \hspace{0.2cm} \text{ for } \hspace{0.2cm} i=1,\dots,p,
\end{equation}
with rate $\sigma(n) \to 0$ as $n \to \infty$.
\end{ass}

Note that the estimators $A_{i,n}$ and $\widehat{\Sigma}_{i,n}$ also depend on the sample size $n$, but for notational simplicity we will omit it in later expressions and simplify as $A_i = A_{i,n}$ and $\widehat{\Sigma}_i = \widehat{
\Sigma}_{i,n}$. In addition, we assume the sequence $c(n)$ to be the same for all $i = 1, \dots, p$, in the following analysis but the extension to the general case is straightforward. Furthermore, we assume the following:
%\begin{ass}\label{ass:distinct}
%Each $M_i$, $i=1,\dots,p$, can be diagonalized with real eigenvectors and $d$ %distinct non-zero real eigenvalues.
%\end{ass}
\begin{ass}\label{ass:distinct}
Each $M_i$, $i=1,\dots,p$ is diagonalizable with real eigenvalues.
\end{ass}

%5The following remarks comment on the previous assumptions and give some insights in upcoming challenges. 

The theoretical results of this paper require no a priori assumptions on the rank of the limiting covariance matrices in \autoref{ass:normal}. In particular, the matrices $\Sigma_{i}$, $i = 1, \dots, p$, in \eqref{eq:normal} may be less than full rank. On one hand, this allows for flexibility in the choice of estimators $A_{i,n}$, on the other hand it elevates the difficulties in deriving asymptotic results for our test statistics. In particular, we aim to give tractable versions of our test statistics in the sense that the covariance matrices can be estimated. While \autoref{ass:covConsistent} ensures the existence of a consistent estimator, one still has to address a potential singularity. We refer to \autoref{subsec:Cov} for a discussion and workaround.



\vspace{-0.5cm}
\subsection{Covariance estimation} \label{subsec:Cov}
\vspace{-0.3cm}
Our test statistics involve the inverses and ranks of the limiting covariance matrices in \autoref{ass:normal}. Ideally, given exact covariance matrices, we can use the so-called Moore-Penrose inverse to address possible singularity. In order to make the statistics tractable in practice, we need to instead incorporate their consistent estimates as given by \autoref{ass:covConsistent}. However, neither the Moore-Penrose inverse nor the rank of a matrix are continuous. 
%%We also include our workaround to deal with the generalized Wald test with the truncated singular value decomposition.

To circumvent those issues, we introduce the so-called truncated singular value decomposition following \cite{lutkepohl1997modified}.
%that will be used to deal with the generalized Wald test with singular covariance matrices. We will then refer to \cite{lutkepohl1997modified} and show the workaround that we will use in our later implementation.
%First we introduce the truncated singular value decomposition that will be used to deal with the generalized Wald test with singular covariance matrices. We will then refer to \cite{lutkepohl1997modified} and show the workaround that we will use in our later implementation.
For an arbitrary matrix $\Psi \in \mathbb{R}^{a \times b}$ with $\tau = \min(a, b) > 1$, its singular value decomposition (SVD) is given by $\Psi = U \Pi W'$ with orthogonal singular vectors $U$ and $W$, and non-increasing, non-negative singular values $\Pi = \diag(\varpi_1, \dots, \varpi_\tau)$.
We define its truncated singular value decomposition with respect to a threshold $\varepsilon \geq 0$ as 
\begin{equation} \label{eq:truSVD}
\Psi(\varepsilon) = U \Pi(\varepsilon) W'
\hspace{0.2cm}
\text{ with }
\hspace{0.2cm}
\Pi(\varepsilon) = \diag(\varpi_1 \mathbb{I}(\varpi_1 > \varepsilon), \dots, \varpi_\tau \mathbb{I}(\varpi_\tau > \varepsilon))
\end{equation}
with indicator function $\mathbb{I}(\cdot)$. In addition, we denote the Moore-Penrose general inverse as $\Psi^+(\varepsilon) = \big(\Psi(\varepsilon)\big)^+$ and the rank function $\rank(\Psi; \varepsilon) = \rank\big(\Psi(\varepsilon)\big)$. 

With the help of the truncated SVD \eqref{eq:truSVD}, we introduce the following lemma that gives consistent estimates for the Moore-Penrose inverse and the rank of a consistently estimated matrix.
\begin{lem}\label{lem:ginverse}
Assume $\widehat{\Sigma}$ is a consistent estimator of a positive semidefinite matrix $\Sigma \in \mathbb{R}^{\tau \times \tau}$, $\varepsilon > 0$ is a constant and not an eigenvalue of $\Sigma$. Then, with $\Sigma(\varepsilon)$ and $\widehat{\Sigma}(\varepsilon)$ defined to be the corresponding truncated SVDs \eqref{eq:truSVD}, 
\begin{equation}\label{eq:wald.cmt}
    \widehat{\Sigma}(\varepsilon) \xrightarrow{\mathcal{P}} \Sigma(\varepsilon), ~ \widehat{\Sigma}^+(\varepsilon) \xrightarrow{\mathcal{P}} \Sigma^+(\varepsilon), ~ \rank(\widehat{\Sigma}; \varepsilon) \xrightarrow{\mathcal{P}} \rank(\Sigma; \varepsilon).
\end{equation}
\end{lem}

The proof of \autoref{lem:ginverse} can be found in {\color{cyan} Appendix D}.
Results in the manner of \autoref{lem:ginverse} are used to circumvent singularity issues which usually occur under the usage of Wald type tests; see %\autoref{app:est} and
\cite{hadi1990note, ratsimalahelo2001rank}.
%\cite{Moore1977, Andrews1987, hadi1990note, ratsimalahelo2001rank}.
The following remark comments on the choice of the threshold $\varepsilon$ in \eqref{eq:wald.cmt}.

\begin{rem}\label{rem:epsilon.order}
With the additional \autoref{ass:covConsistent} that $\widehat{\Sigma}$ converges with rate $O_\mathcal{P}\big(\sigma(n)\big)$, the threshold $\varepsilon$ that satisfies $\varepsilon = o(1)$ and $\sigma(n)/\varepsilon = o(1)$ as $n \to \infty$ can be chosen to optimize the accuracy of the generalized inverse and rank estimators. See p.\ 320 in \cite{lutkepohl1997modified} for a discussion on the choice of $\varepsilon$.
\end{rem}


%\section{Two-sample test}
%\label{sec:2sample}

%We start from a two-sample test $p = 2$. In \autoref{sec:commute}, we design a test statistic based on the commutator of the two matrices under consideration. In \autoref{sec:llrt}, the log-likelihood ratio test is adjusted and in \autoref{sec:llrintro} a test statistic for the hypothesis testing problem \eqref{hyp:all} is introduced, along with its sensitivity analysis in \autoref{sec:error}.


%\subsection{Commutator-based statistic}
%\label{sec:commute}

\vspace{-0.5cm}
\section{Two-sample test}\label{sec:commute}
\vspace{-0.3cm}
We start from a two-sample test $p = 2$ and design a test statistic based on the commutator of the two matrices under consideration. 
Under \autoref{ass:distinct}, matrices commute if and only if they can be diagonalized simultaneously; see Theorem 1.3.12 in \cite{Horn}. Hence, one intuitive idea to measure how far $M_1$ and $M_2$ are from being commutable is, to calculate some form of metric of their commutator $[M_1, M_2] := M_1 M_2-M_2 M_1$. The following proposition introduces a statistic to test the hypothesis \eqref{hyp:all} and provides its asymptotic behavior. 
\begin{prop}\label{thm:comm}
Suppose \Cref{ass:normal,ass:distinct} are satisfied and denote $\bm{\eta}_n = \Vector[A_1, A_2]$. Then, under $H_0$ in \eqref{hyp:all},
\begin{equation}\label{eq:comm.asym}
    c(n) \bm{\eta}_n \xrightarrow{\mathcal{D}} \mathcal{N}(0, \Sigma_\eta)
\end{equation}
with $\Sigma_\eta = \Sigma_{1,2} + \Sigma_{2,1}$, where $\Sigma_{k, \ell} = \Lambda(M_\ell) \Sigma_k \Lambda'(M_\ell)$ for $k \ne \ell$, and $\Lambda(X) = I_d \otimes X - X' \otimes I_d$ is a function in $X \in \R^{d \times d}.$ Then,
\begin{equation}\label{eqn:gamma2}
    \Gamma_1 := c^2(n) \bm{\eta}_n' \Sigma_\eta^+ \bm{\eta}_n \xrightarrow{\mathcal{D}} \chi^2(r_1),
\end{equation}
where $r_1$ is the rank of $\Sigma_\eta$.
\end{prop}

The proof of \autoref{thm:comm} can be found in {\color{cyan} Appendix D}. In order to make \autoref{thm:comm} tractable in practice, we can obtain a consistent estimator $\widehat{\Sigma}_\eta$ by substituting $\Sigma_{i}, M_{i}$, $i=1,2$, in the expression of $\Sigma_\eta$ with $A_{i},\widehat{\Sigma}_{i}$, $i=1,2$, in \Cref{ass:normal,,ass:covConsistent}, respectively. The consistency is verified by the continuous mapping theorem and \Cref{ass:normal,ass:covConsistent} as
$$
\widehat{\Sigma}_\eta - \Sigma_\eta = \Lambda(A_2) \widehat{\Sigma}_1 \Lambda'(A_2) + \Lambda(A_1) \widehat{\Sigma}_2 \Lambda'(A_1) - \Sigma_\eta \xrightarrow{\mathcal{P}} 0.
$$
Note that $\Sigma_\eta$ and $\widehat{\Sigma}_\eta$ are both singular matrices as there exists at least one non-trivial vector $\bm{v} = \Vector(I_d)$ such that $\Sigma_\eta \bm{v} = \widehat{\Sigma}_\eta \bm{v} = 0$, since
$$\Lambda(X)\bm{v} = (I_d \otimes X)\bm{v} - (X' \otimes I_d)\bm{v} =0,$$
where the last equality follows by Theorem 2 in \cite{magnus2019matrix}, p.\ 35.
Hence $r_1$ and $\hat{r}_1$, the ranks of $\Sigma_\eta$ and $\widehat{\Sigma}_\eta$ respectively, are always less than $d^2$. Due to the singularity issue
%non-continuity of the Moore-Penrose inverse and the rank 
in \eqref{eqn:gamma2}, we propose to use the truncated version \eqref{eq:wald.cmt} of $\widehat{\Sigma}_\eta$.

\begin{prop}\label{thm:comm.Est}
Suppose \Cref{ass:normal,,ass:covConsistent,,ass:distinct} are satisfied. Then, for a given threshold $\varepsilon > 0$ that is not an eigenvalue of $\Sigma_{\eta}$ (defined through \eqref{eq:comm.asym}),
%given \eqref{eq:comm.asym} holds, 
the test statistic is defined as and satisfies
\begin{equation}\label{eq:gamma2.est}
    \Gamma_1^\#(\varepsilon) := c^2(n) \bm{\eta}_n' \widehat{\Sigma}_\eta^+(\varepsilon) \bm{\eta}_n \xrightarrow{\mathcal{D}} \chi^2(\widehat{r}_1(\varepsilon)),
\end{equation}
where $\widehat{r}_1(\varepsilon) = \rank(\widehat{\Sigma}_\eta; \varepsilon)$.% In addition, the threshold $\varepsilon$ that satisfies $\varepsilon = o(1)$ and $\sigma(n)/\varepsilon = o(1)$ can be chosen to optimize the test power.
\end{prop}



%can use available matrix rank estimators as introduced in \cite{Kleibergen2006:Generalized} and studied for symmetric matrices in \cite{donald:2007rank}.
%and consequently $\hat{r} - r \xrightarrow{\mathcal{P}} 0$.





\vspace{-0.5cm}
\section{Multi-sample test}\label{sec:multi}
\vspace{-0.3cm}
An extension of the topic introduced in \autoref{sec:commute} is to conduct the test on a larger pool of matrices ($p \ge 2$). % Two different approaches are considered here. Instead of focusing on the individual test result for each pair of matrices, we are more interested in whether the same hypothesis holds simultaneously across the whole pool of matrices.
%
In addition to testing simultaneously over all pairs of samples using the methods based on the two-sample test introduced in \autoref{sec:commute}, we are more interested in whether the same hypothesis holds across the whole pool of matrices. With that in mind, we propose to use the estimated optimal common eigenvectors and test whether they annihilate the off-diagonal elements of the matrices after transformation.

For the simultaneous test, with the commutator-based test developed in \autoref{sec:commute}, the approach and the test statistic are straightforward. For instance, a matrix of test statistics (or p-values) represents the pairwise test results and conclusions can then be drawn. Hence, we will omit further details here and focus on the more comprehensive approach regarding whether the same hypothesis holds across the whole pool of matrices. Specifically, we will refer to optimization algorithms for calculating the common eigenvectors that almost diagonalize a pool of matrices (see \autoref{subsec:common}) and then design a test statistic (see \autoref{subsec:eigtest}).

\vspace{-0.5cm}
\subsection{Common eigenvectors finder}\label{subsec:common}
\vspace{-0.3cm}
In this section, we briefly introduce the setup of the optimization problem of finding the optimal diagonalizer for a pool of matrices.% Other than considering optimization routines for symmetric matrices proposed by \cite{Fuji, Ghazi, Gira}, we are more interested in approximating common eigenvectors for general asymmetric matrices. Some previous ideas, like '\textit{sh-rt}' by \cite{shrt}, '\textit{JUST}' by \cite{just}, and '\textit{(W)JDTE}' by \cite{jdtm}, are shown to be effective numerically in their original papers and ready for implementation.

Recall our setup, with a pool of matrix-valued statistics $\mathcal{A} = \{A_i\}_{i = 1}^p$ estimating the matrices $\mathcal{M} = \{M_i\}_{i = 1}^p$. We refer to the '\textit{(W)JDTE}' algorithm by \cite{andre} due to its performance in terms of speed and accepted accuracy. The algorithm provides the common eigenvectors of a pool of matrices by minimizing the objective function
\begin{equation}
    \text{off}(U; \mathcal{A}) = \sum_{i = 1}^p \text{off}_2(U^{-1} A_i U).
\end{equation}
Here, $\text{off}_2(X) = \sum_{i \ne j} |X_{i,j}|^2$ denotes the off-diagonal sum-of-squares for $X \in \mathbb{R}^{d \times d}$. For the details of the algorithm; see \cite{andre}.

\vspace{-0.5cm}
\subsection{Eigenvector test}\label{subsec:eigtest}
\vspace{-0.3cm}
In this section, we propose a test for $H_0$ in \eqref{hyp:all} allowing the number of matrices $p$ to be larger than two. The test is based on the assumption that an invertible matrix $V \in \R^{d \times d}$ is given as a guess for the common eigenvector matrix. 
%We see the below stated theoretical results as a starting point
We acknowledge that the assumption of knowing the matrix $V$ is quite restrictive. For all practical purposes, we refer to the optimization problem in \autoref{subsec:common} which provides an optimal diagonalizer based on the matrix estimators $\mathcal{A} = \{A_i\}_{i = 1}^p$.
From a theoretical perspective, existing literature does not suggest any formal testing procedure based on explicit estimators for $V$. In particular, there are no asymptotic results, neither under the assumption that the matrices to be tested are asymmetric nor symmetric. For this reason, we see the below stated theoretical results under the assumption that $V$ is known as a starting point and leave a more rigorous investigation for future work.

%Existing literature on simultaneous diagonalizability lacks theoretical verification. 

Define the function
$
\offvec_d: \R^{d \times d} \to \R^{d(d - 1)}, ~ \offvec_d(X) = S_d \Vector(X),
$
which stacks all off-diagonal elements of a square matrix $X$ with dimension $d$ columnwise. We will always use $S_d$ as the off-diagonal selection matrix for square matrices of dimension $d$. The test can then be designed as follows.

\begin{prop}\label{thm:multi_eig}
Suppose \Cref{ass:normal,ass:distinct}, and $V$ is given as a guess for the common eigenvector matrix. Let 
\begin{equation} \label{eq:xii}
\bm{\zeta}_i = \offvec_d(V^{-1} A_i V), \hspace{0.2cm} i = 1, \dots, p, 
\hspace{0.2cm}\text{and}\hspace{0.2cm} 
S_{V,d} = S_d (V' \otimes V^{-1}). 
\end{equation}
Then, under $H_0$ in \eqref{hyp:all},
%\label{eq:multi_asym}
$
    c(n) \bm{\zeta}_i \xrightarrow{\mathcal{D}} \mathcal{N}(0, \Theta_i)
$
with $\Theta_i = S_{V,d} \Sigma_i S_{V,d}'$ for $i = 1, \dots, p$.
The test statistic is defined as and satisfies
\begin{equation}\label{eqn:multi_wald}
    \Gamma_3 := c^2(n) \sum_{i = 1}^p \bm{\zeta}_i' \Theta_i^+ \bm{\zeta}_i \xrightarrow{\mathcal{D}} \chi^2(r_3),
\end{equation}
where $r_3 = \sum_{i=1}^p \rank(\Theta_i)$.
\end{prop}

Note that in practice, the eigenvector matrix to be tested is always obtained from optimization, and hence this idea highly depends on the accuracy of such algorithms.
%In later simulations we do see unavoidable errors influencing the test power when plugging in the estimated optimizer of $V$ from '\textit{(W)JDTE}'.
In order to reduce the influence of estimation errors, we develop the following analogous test that tolerates relatively larger errors while maintaining acceptable efficiency.
\begin{cor}\label{cor:multi}
Under the assumptions of \autoref{thm:multi_eig}, let $\Theta = \blkdiag(\{\Theta_i\}_{i = 1}^p)$ be a block-diagonal matrix, and
$
\bm{\zeta} := \big(\bm{\zeta}_1', \dots, \bm{\zeta}_p' \big)' \in \mathbb{R}^{p(d^2-d)},
$ with $\bm{\zeta}_{i}$ as in \eqref{eq:xii} such that $c(n) \bm{\zeta} \xrightarrow{\mathcal{D}} \mathcal{N}(0, \Theta)$. Then, the test statistic is defined as and satisfies
%\begin{equation}\label{eqn:multi_norm}
%    \Gamma_3^* = c^2(n) \|\bm{\zeta}\|^2 = c^2(n) \sum_{i=1}^p \|\bm{\zeta}_i\|^2
%    \xrightarrow{\mathcal{D}}
%    \sum_{i=1}^{p} \sum_{r=1}^{d^2-d} \lambda_{r}(\Theta_{i}) \chi^2(1) .
%\end{equation}
\begin{equation}\label{eqn:multi_norm}
    \Gamma_3^* := c^2(n) \sum_{i=1}^p \|\bm{\zeta}_i\|^2
    \xrightarrow{\mathcal{D}} \psi_3^* \coloneqq
    \sum_{r=1}^{p(d^2-d)} \lambda_{r}(\Theta) \chi^2(1),
\end{equation}
where $\lambda_{r}(\Theta)$ denotes the $r$th eigenvalue of $\Theta$. Furthermore, the p-value based on \eqref{eqn:multi_norm} can be approximated by
\begin{equation}\label{eqn:full_approx}
    \mathbb{P}(\psi_3^* > \Gamma_3^* ~|~ H_0) \approx \mathbb{P}(\gamma_3^* > \Gamma_3^* ~|~ H_0), \hspace{0.2cm} \mbox{where} \hspace{0.2cm} \gamma_3^* \sim Gamma\Big(\frac{\tr(\Theta)^2}{2 \tr(\Theta^2)}, \frac{\tr(\Theta)}{2 \tr(\Theta^2)}\Big).
\end{equation}
% and $\sim$ an approximation by a gamma distribution.
%Then its limiting distribution, which originally converges to a weighted sum of chi-squared distributions, can be approximated by a gamma distribution:
%\begin{equation}\label{eqn:multi_alt}
%%    \Gamma_3^* \xrightarrow{\mathcal{D}} 
%\gamma\Big(\frac{\tr(\Sigma)^2}{2 \tr(\Sigma^2)}, \frac{\tr(\Sigma)}{2 \tr(\Sigma^2)}\Big).
%\end{equation}
\end{cor}
The approximation in \eqref{eqn:full_approx} between $\psi_3^*$, a weighted sum of chi-squared distributed random variables, and $\gamma_3^*$, a gamma distribution, was introduced in \citet[Theorem 3.1]{box1954} based on matching first- and second-order moments of the two distributions. For accuracy analysis, see \cite{bodenham2016comparison}.

Note, that \autoref{thm:multi_eig} requires to calculate the generalized inverse of $\Theta$.
In contrast, \autoref{cor:multi} only needs the trace of $\Theta$. Calculating the trace of $\Theta$ is expected to be more robust towards estimation errors of the eigenvector matrix $V$ than finding the generalized inverse of $\Theta$.

In practice, only the estimated $\widehat{\Sigma}_i$ are accessible. Due to possible singularity issues with the general inverse $\Theta_i^+$ in \autoref{thm:multi_eig}, we use results in {\color{cyan} Appendix E} to introduce the following proposition.

\begin{prop}\label{thm:multi_eig.Est}
Suppose \Cref{ass:normal,,ass:covConsistent,,ass:distinct}. Then, for a given threshold $\varepsilon > 0$ that is not an eigenvalue of $\Theta_i$ for $i = 1, \dots, p$, 
%given \eqref{eq:multi_asym} holds, 
the test statistic is defined as and satisfies
\begin{equation}\label{eq:gamma3.est}
    \Gamma_3^\#(\varepsilon) := c^2(n) \sum_{i = 1}^p \bm{\zeta}_i' \widehat{\Theta}_i^+(\varepsilon) \bm{\zeta}_i \xrightarrow{\mathcal{D}} \chi^2(\widehat{r}_3(\varepsilon)),
\end{equation}
where $\widehat{\Theta}_i = S_{V, d} \widehat{\Sigma}_i S_{V, d}$ and $\widehat{r}_3(\varepsilon) = \sum_{i=1}^p \rank(\widehat{\Theta}_i; \varepsilon)$.% In addition, the threshold $\varepsilon$ that satisfies $\varepsilon = o(1)$ and $\sigma(n)/\varepsilon = o(1)$ can be chosen to optimize the test power.
\end{prop}

\vspace{-0.5cm}
\section{Partial test}\label{sec:part}
\vspace{-0.3cm}
In this section, we focus on the hypothesis $H_0^{*}$ in \eqref{hyp:part}. We first reformulate the hypothesis testing problem (\autoref{sec:partialprob}) and then design the corresponding test statistics and analyze their asymptotic behavior (\autoref{sec:partialtest}). Meanwhile, an optimization algorithm is also proposed to approximate the matrix $V$ in the statement of $H_0^{*}$ (\autoref{sec:partalg}).

\vspace{-0.5cm}
\subsection{Problem representation} \label{sec:partialprob}
\vspace{-0.3cm}
Suppose $\{M_i\}_{i = 1}^p$ satisfy $H_0^{*}$, then for $V$ specified in the hypothesis, there exists an orthogonal $d \times k$ matrix $Q_k$ (i.e. $Q_k' Q_k = I_k$), such that $V = Q_k R$, where $R$ is a $k \times k$ upper-triangular matrix. By orthogonally spanning $Q_k = (\bm{q}_1, \dots, \bm{q}_k)$ to $Q = (\bm{q}_1, \dots, \bm{q}_d) \in \mathbb{R}^{d \times d}$, we have
\begin{equation}
   Q' M_i Q = \begin{pmatrix} R D_i R^{-1} & * \\ 0 & ** \end{pmatrix} ~ =: ~ \begin{pmatrix} \Phi_i & * \\ 0 & ** \end{pmatrix}\label{eqn:part}
\end{equation}
with $D_{i}$, $i=1,\dots,p$, as in \eqref{hyp:part}.
We use the symbols $*$ and $**$ to denote non-zero block matrices with proper dimensions which are not relevant. Note that the set of upper-triangular matrices $\{\Phi_i\}_{i = 1}^p$ shares the common eigenvectors $R$. For the estimators $\mathcal{A} = \{A_i\}_{i = 1}^p$ given by \autoref{ass:normal}, suppose a matrix $\widehat{Q}$ is given as a guess for $Q$, such that
\begin{equation}
    \widehat{Q}' A_i \widehat{Q} = \begin{pmatrix} B_i & * \\ C_i & ** \end{pmatrix},\label{eqn:part_noisy}
\end{equation}
where $B_i \in \mathbb{R}^{k \times k}$ and $C_i \in \mathbb{R}^{(d-k) \times k}$. 
The matrix $\widehat{Q}$ can either be given by knowing the ground-truth $Q$ or by estimation; see \autoref{sec:partalg} for an algorithm to estimate $Q$.
The procedure of our test starts from finding the orthogonal matrix $Q$ that contains the first $k$ columns as the common invariant subspace, and designing tests on the transformed matrices $\{B_i, C_i\}_{i = 1}^p$. The test is a combination of:
\begin{enumerate}
    \item Whether the transformation by the orthogonal matrix $\widehat{Q}$ introduces the all-zero lower-left block in \eqref{eqn:part} by testing on $\{C_i\}_{i = 1}^p$.
    
    \item Whether the $k \times k$ matrices $\{\Phi_i\}_{i = 1}^p$ defined in \eqref{eqn:part} satisfy $H_0$ in \eqref{hyp:all} by testing on $\{B_i\}_{i = 1}^p$.
\end{enumerate}
To introduce our test statistic, suppose $\widehat{Q}$ is given, and let
\begin{equation*}
\widehat{Q}_{BC} = \begin{pmatrix} S_B \\ S_C \end{pmatrix} (\widehat{Q}' \otimes \widehat{Q}')
\hspace{0.2cm}
\text{ with }
\hspace{0.5cm}
\begin{matrix}
S_B \mbox{vec}(\widehat{Q}' A_i \widehat{Q}) = \mbox{vec}(B_i), \\
S_C \mbox{vec}(\widehat{Q}' A_i \widehat{Q}) = \mbox{vec}(C_i),
\end{matrix}
\end{equation*}
where $S_B$ and $S_C$ are selection matrices defined 
%by equations
%$$
%S_B \mbox{vec}(\widehat{Q}' A_i \widehat{Q}) = \mbox{vec}(B_i), ~ S_C %\mbox{vec}(\widehat{Q}' A_i \widehat{Q}) = \mbox{vec}(C_i)
%$$
according to \eqref{eqn:part_noisy}. Hence, the partially common eigenvectors of $\{M_i\}_{i = 1}^p$ can be estimated as $\widehat{V} = \widehat{Q}_k \widetilde{V}$, where $\widehat{Q}_k$ is a matrix of the first $k$ columns of $\widehat{Q}$ and $\widetilde{V}$ the estimated common eigenvectors of $\{B_i\}_{i = 1}^p$. Referring to \autoref{subsec:common}, $\widetilde{V}$ can be received from '\textit{(W)JDTE}'. So far, we have supposed that $\widehat{Q}_k$ is a guess. However, it can be estimated by an algorithm proposed in \autoref{sec:partalg} below.


%And referring to \autoref{subsec:common}, denote the common eigenvectors of $\{B_i\}_{i = 1}^p$ estimated from '\textit{(W)JDTE}' to be $\widetilde{V}$. Hence, the partially common eigenvectors are estimated as $\widehat{V} = \widehat{Q}_k \widetilde{V}$, where $\widehat{Q}_k$ is a matrix of the first $k$ columns of $\widehat{Q}$. 

\vspace{-0.5cm}
\subsection{Partial eigenvector test}\label{sec:partialtest}
\vspace{-0.3cm}
In this section, we introduce our test statistic to test for partially common eigenvectors and present their asymptotic behavior.

\begin{prop}\label{thm:part}
Suppose \Cref{ass:normal,ass:distinct} are satisfied and let
\begin{equation}\label{eqn:wii}
\bm{w}_i =  P_w \Vector(A_i) 
\hspace{0.2cm} \text{ with } \hspace{0.2cm}
P_w := \begin{pmatrix} S_{\widetilde{V}, k} & 0 \\ 0 & I_{k(d-k)} \end{pmatrix} ~ Q_{BC},
\end{equation}
where $S_{\widetilde{V}, k}$ is defined as in \eqref{eq:xii} and $Q_{BC}$ is analogous to $\widehat{Q}_{BC}$ except replacing the guess $\widehat{Q}$ with true $Q$.
Then, under $H_{0}$ in \eqref{hyp:all},
$
    c(n) \bm{w}_i \xrightarrow{\mathcal{D}} \mathcal{N}(0, \Omega_i) \label{eqn:partnorm}
$
with $\Omega_i = P_w \Sigma_i P_w'$ for $i = 1, \dots, p$. The test statistic is defined as and satisfies
\begin{equation}
    \Gamma_4 := c^2(n) \sum_{i = 1}^p \bm{w}_i' \Omega_i^+ \bm{w}_i \xrightarrow{\mathcal{D}} \chi^2(r_4),\label{eqn:partchi}
\end{equation}
where $r_4 = \sum_{i=1}^p \rank(\Omega_i)$.
\end{prop}

And similar to \autoref{cor:multi}, we design the following test statistic with p-value approximated by a gamma distribution.
\begin{cor}\label{cor:partial}
Under the assumptions in \autoref{thm:part}, let $\Omega = \blkdiag(\{\Omega_i\}_{i = 1}^p)$ be a block-diagonal matrix, and
$
\bm{w} := \big(\bm{w}_1', \dots, \bm{w}_p' \big)' \in \mathbb{R}^{pk(d-1)},
$ with $\bm{w}_{i}$ as in \eqref{eqn:wii} such that $c(n) \bm{w} \xrightarrow{\mathcal{D}} \mathcal{N}(0, \Omega)$. Then the test statistic is defined as and satisfies
\begin{equation}\label{eqn:partgam}
    \Gamma^*_4 := c^2(n) \|\bm{w}\|^2
    \xrightarrow{\mathcal{D}} \psi_4^* \coloneqq
    \sum_{r=1}^{pk(d-1)} \lambda_{r}(\Omega) \chi^2(1),
\end{equation}
where $\lambda_r(\Omega)$ denotes the $r$th eigenvalue of $\Omega$. Furthermore, the p-value based on \eqref{eqn:partgam} can be approximated by
\begin{equation}\label{eqn:part_approx}
    \mathbb{P}(\psi_4^* > \Gamma_4^* ~|~ H_0) \approx \mathbb{P}(\gamma_4^* > \Gamma_4^* ~|~ H_0), \hspace{0.2cm} \mbox{where} \hspace{0.2cm} \gamma_4^* \sim Gamma\Big(\frac{\tr(\Omega)^2}{2 \tr(\Omega^2)}, \frac{\tr(\Omega)}{2 \tr(\Omega^2)}\Big).
\end{equation}
\end{cor}

Similar as \autoref{thm:multi_eig.Est}, we use results in {\color{cyan} Appendix E} for the following tractable version of \autoref{thm:part} that takes care of possible singularity issues with $\Omega_i^+$ in \eqref{eqn:partchi}.

\begin{prop}\label{thm:part.Est}
Assume \Cref{ass:normal,,ass:covConsistent,,ass:distinct} are satisfied. Then, for a given threshold $\varepsilon > 0$ that is not an eigenvalue of $\Omega_i$ for $i = 1, \dots, p$, 
%given \eqref{eqn:partnorm} holds, 
the test statistic is defined as and satisfies
\begin{equation}\label{eq:gamma4.est}
    \Gamma_4^\#(\varepsilon) := c^2(n) \sum_{i = 1}^p \bm{w}_i' \widehat{\Omega}_i^+(\varepsilon) \bm{w}_i \xrightarrow{\mathcal{D}} \chi^2(\widehat{r}_4(\varepsilon)),
\end{equation}
where $\widehat{\Omega}_i = P_w \widehat{\Sigma}_i P_w'$ for $i = 1, \dots, p$, and $\widehat{r}_4(\varepsilon) = \sum_{i=1}^p \rank(\widehat{\Omega}_i; \varepsilon)$.% In addition, the threshold $\varepsilon$ that satisfies $\varepsilon = o(1)$ and $\sigma(n)/\varepsilon = o(1)$ can be chosen to optimize the test power.
\end{prop}

\vspace{-0.5cm}
\subsection{Optimization algorithm} \label{sec:partalg}
\vspace{-0.3cm}
This section is dedicated to finding an estimator $\widehat{Q}$ for $Q$ in \eqref{eqn:part}.
In implementation, $\widehat{Q}$ can be obtained by minimizing the following objective function
$$
f(Q; \mathcal{A}, k) = \sum_{i = 1}^p \sum_{r = k+1}^d \sum_{s = 1}^k (\bm{q}_r' A_i \bm{q}_s)^2 ~ \mbox{ subject to } \bm{q}_r' \bm{q}_s = \delta_{rs}, ~ \forall~ r,s = 1, \dots, d,
$$
where $Q = (\bm{q}_1, \dots, \bm{q}_d) \in \mathbb{R}^{d \times d}$.
\cite{tensor} introduced a version of Gauss-Newton algorithm for the joint Schur decomposition based on matrix exponential, and showed its global minimum guarantees if the initial value is sufficiently close to the ground-truth $Q$. In our work, we inherit the idea of this algorithm with slight revision, where the major difference is to substitute the selection matrix from lower-triangular indicator to the $(d-k) \times k$ lower-left block indicator.

We also introduce a warm-up algorithm to supply the initial values for this Gauss-Newton approach. Since the matrices $\{\Phi_i\}_{i = 1}^p$ in \eqref{eqn:part} are upper-triangular, we can split the minimization with respect to $Q$ into sequentially optimizing each column $\bm{q}_r$ with $r$ from $1$ to $k$ based on the following objective function
$$
f^*(Q; \mathcal{A}, k) = \sum_{i = 1}^p \sum_{s = 1}^k \sum_{r = s+1}^d (\bm{q}_r' A_i \bm{q}_s)^2, \mbox{ with } Q = (\bm{q}_1, \dots, \bm{q}_d) \in \mathbb{R}^{d \times d}.
$$
More precisely, we introduce the following \autoref{algo:Qhat} as the whole process for optimizing orthogonal $\widehat{Q}$, including the initialization warm-up before \autoref{step:GN}.

%\begin{algorithm}[htbp]
%    \caption{Optimization of $Q$}\label{algo:Qhat}
%    \textbf{Input:} Estimators $\mathcal{A} = \{A_i\}_{i=1}^p$, number of common %eigenvectors $k$.\\
%    Initialize $\widehat{Q} = I_d = (\widehat{\bm{q}}_1, \dots, \widehat{\bm{q}}_d)$ as %an identity matrix.\\
%    \For{$i = 1:k$}{
%        Optimize $\mathcal{O}_i = (\bm{p}_i, P_i) = \argmin_Q f(Q; \mathcal{A}, 1)$ %where $\bm{p}_i \in \mathbb{R}^{d-i+1}$ and $P_i \in \mathbb{R}^{(d-i+1) \times %(d-i)}$.\label{algo:opt}\\
%        Update $(\widehat{\bm{q}}_i, \dots, \widehat{\bm{q}}_d) = (\widehat{\bm{q}}_i, %\dots, \widehat{\bm{q}}_d) \mathcal{O}_i$.\\
%        Update $\mathcal{A}$ by $A_j = P_i' A_j P_i$ for $j = 1, \dots, p$.
%    }
%    Input $\widehat{Q}$ into the modified version of Gauss-Newton approach by %\cite{tensor} as the initial value and get the final output $\widehat{Q}$ %there.\label{step:GN}\\
%    \textbf{return} $\widehat{Q} = (\widehat{\bm{q}}_1, \dots, \widehat{\bm{q}}_d)$.
%\end{algorithm}
\begin{algorithm}[htbp]
    \caption{Estimation of $Q$}\label{algo:Qhat}
    \KwIn{Estimators $\mathcal{A} = \{A_i\}_{i=1}^p$, number of common eigenvectors $k$.}
    \KwOut{Estimator $\widehat{Q}$ for $Q$.}
    Initialize $\mathcal{O}_0 = I_d $ as an identity matrix.\\
    \For{$j = 1:k$}{
        Optimize $\mathcal{O}_j = \argmin_Q f(Q; \mathcal{A}, 1)$ and write $\mathcal{O}_j = (\bm{p}_j, P_j)$, where $\bm{p}_j \in \mathbb{R}^{d-j+1}$ and $P_j \in \mathbb{R}^{(d-j+1) \times (d-j)}$. \label{algo:opt}\\
        Update $\mathcal{O}_{j-1} = \mathcal{O}_{j-1} \mathcal{O}_j$.\\
        Update $\mathcal{A}$ by $A_i = P_j' A_i P_j$ for $i = 1, \dots, p$.
    }
    Input $\mathcal{O}_k$ into the modified version of Gauss-Newton approach by \cite{tensor} as the initial value and get the final output $\widehat{Q}$.\label{step:GN}
    %\textbf{return} $\widehat{Q}$.
\end{algorithm}
For realization of the optimization on \autoref{algo:opt}, we refer to the FG-algorithm by \cite{Flury86}.


\vspace{-0.5cm}
\section{Simulation study}\label{sec:simu}
\vspace{-0.3cm}
Simulation studies are run for two-sample, multi-sample, and partial tests. The barplots of p-values from multiple replicates, especially, the type I and type II errors which are readable from the plots of histograms, are given as evidence for the effectiveness of our test designs. Complementary to the plots presented in this section, we refer the reader to {\color{cyan} Section A.1} in the supplementary material for tables providing type I and II errors for all our simulation studies.

In addition, instead of implementing \Cref{thm:comm,,thm:multi_eig,,thm:part}, we always turn to their truncated versions (\Cref{thm:comm.Est,,thm:multi_eig.Est,,thm:part.Est} respectively) in \Cref{sec:simu,,sec:app}, in order to take care of possibly singular covariance matrices. Furthermore, since the following examples with sample size $n$ all have the same rate of convergence $\sigma(n) = n^{-1/2}$ for their limiting covariance matrix estimators, we will not further specify but choose $\varepsilon = n^{-1/3}$ by default whenever applicable in implementations. This choice of $\varepsilon$ is also in accordance with the discussion in \autoref{rem:epsilon.order} and maintains the test power.

\vspace{-0.5cm}
\subsection{Data generating process (DGP)}\label{subse:DGP}
\vspace{-0.3cm}
Prior to our testing analysis, we first introduce the data generating process (DGP) for our targeted matrices $\mathcal{M}_p(\rho, k; d) = \{M_i(\rho, k; d)\}_{i=1}^p$ with parameters $\rho$ accounting for the signal-to-noise variance ratio ($\SNR \coloneqq \frac{1}{\rho^2}$) and $k$ the number of common eigenvectors.




\begin{algorithm}[htbp]
    \KwIn{Number of matrices $p \in \mathbb{N}$, matrix dimension $d \in \mathbb{N}$, number of common eigenvectors $k \in \{1, \dots, d\}$, noise measure $\rho \ge 0$.}
    \KwOut{Pool of matrices $\mathcal{M}_p(\rho, k; d)$ with common eigenvectors.}
    Randomly generate an eigenvector matrix $V(k; d) \in \R^{d \times k}$ ($k \le d$).\label{step:commonV}\\
    Set $V_i(k; d) = V(k; d)$ if $k = d$, otherwise span $V_i(k; d) = (V(k; d), \widetilde{V}_i) \in \R^{d \times d}$ with random but sufficiently distinct $\widetilde{V}_i \in \R^{d \times (d-k)}$ for $i = 1, \dots, p$.\\
    \For{$i=1, \dots, p$}{perturb the $i$-th eigenvector matrix $V_i(k; d)$ as $V_i(\rho, k; d) = V_i(k; d) + \rho E_i$ with noise $E_i$ to be independent and standard normal, element-wise.\label{step:diffV}}
    Randomly generate non-singular diagonal matrices $D_i \in \R^{d \times d}$ for $i = 1, \dots, p$.\\
    Generate the target matrices $\mathcal{M}_p(\rho, k; d) = \{ V_i(\rho, k; d) D_i V_i^{-1}(\rho, k; d)\}_{i=1}^p$.
    %\Return $\mathcal{M}_p(\rho, k; d)$.
    \caption{DGP for $\mathcal{M}_p(\rho, k; d)$}\label{algo:DGP}
\end{algorithm}



With the set of target matrices $\mathcal{M}_p(\rho, k; d)$, we can then proceed to design different sampling setup for their corresponding consistent estimators $\mathcal{A}_p(\rho) = \{A_i(\rho)\}_{i=1}^p$. Note that we define $\SNR = \frac{1}{\rho^2}$ to quantify the similarity among the underlying eigenvectors $\{V_i(\rho, k; d)\}_{i=1}^p$ (see \autoref{step:diffV} of \autoref{algo:DGP}) rather than the estimation accuracy for $\mathcal{A}_p(\rho)$, i.e., if ideally $\SNR = \infty$ or $\rho = 0$, the matrices in $\mathcal{M}_p(\rho, k; d)$ can be perfectly diagonalized with common eigenvectors (partially when $k < d$), while the randomness of our test statistics still exists due to the subsequent estimation for $\mathcal{A}_p(\rho)$.

\vspace{-0.5cm}
\subsection{Two-sample test} \label{subse:2sample}
\vspace{-0.3cm}
With $\mathcal{M}_2(\rho, d; d) = \{M_1(\rho, d; d), M_2(\rho, d; d)\}$ generated following \autoref{algo:DGP} in \autoref{subse:DGP}, we treat them as the mean matrices for two pools of $n$ normally distributed observations and propose the averages of samples as the corresponding estimators $\mathcal{A}_2(\rho)$. The covariance matrices are consistently estimated as well. According to the central limit theorem (CLT), the convergence rate in \autoref{ass:normal} satisfies $c(n) = \sqrt{n}$. If not further specified, we stick to the classical p-value test framework and always reject the null $H_0$ (or $H_0^{*}$) when the p-value is below a $0.05$ significance level.

%Due to the lack of approximation accuracy for the log-likelihood ratio test statistic $\Gamma_2$ in \Cref{thm:LLR,,thm:LLRest}, the simulations are only applied to the commutator-based test in \autoref{thm:comm.Est}.

We set the dimension of the matrices as $d = 5$, $\SNR \in \{10, 1000, \infty\}$, and the sample size $n \in \{50, 250, 1000\}$. For each data-generating setting determined by $(\SNR, n)$, the \autoref{algo:DGP} is repeated $500$ times independently to get the data of $500$ pairs of estimators for the commutator-based test; see \Cref{fig:comm} for performance. Note that the first bar in these figures represents the proportion of the 500 p-values lower than the critical threshold 0.05, i.e., the proportion of simulations that one rejects the null hypothesis $H_0$ based on the test design. Ideally for $\SNR = \infty$, this proportion, also known as type I error rate, should be close to 0.05, which is a typical choice of significance level. For $\SNR \ne \infty$, the proportion of p-values outside the first bar, also known as type II error rate, should approach 0 if the test has high power.

\begin{figure}[h] %  figure placement: here, top, bottom, or page
    \centering
    \centerline{\input{Plots/PvalueCommutator.tikz}}
    \caption{The histograms of p-values for the commutator-based test in \autoref{thm:comm.Est} with simulated data.}
   \label{fig:comm}
\end{figure}

For the commutator-based test \autoref{thm:comm.Est}, we see from \autoref{fig:comm} that with sample size increasing, the p-values of samples from null space ($\SNR = \infty$) tend to be uniformly distributed on the interval $[0,1]$, and the p-values of samples from alternative spaces start to concentrate in the interval $[0, 0.05)$. When the sample size $n$ exceeds a certain level, $200$ for instance, the test performs well with acceptable type I error and excellent type II error.


\vspace{-0.5cm}
\subsection{Multi-sample test} \label{se:sim-multi}
\vspace{-0.3cm}
For the simulation study of the multi-sample test introduced in \autoref{sec:multi}, we follow \autoref{algo:DGP} to set the mean matrices of normally distributed observations as $\mathcal{M}_p(\rho, d; d)$. We consider here the setting that dimension $d = 4$, $p = 8$, $\SNR = \frac{1}{\rho^2} \in \{10, 100, 1000, \infty\}$, and sample size $n \in \{10^2, 10^3, 10^4, 10^5\}$. Each consistent estimate in $\mathcal{A}(\rho)$ is the empirical average from the $n$ observations, hence $c(n) = \sqrt{n}$ by the CLT. Note that when $\rho = 0$, equivalently $\SNR = \infty$, the matrices in $\mathcal{A}_p(\rho)$ satisfy $H_0$, i.e., share common eigenvectors.

To verify the testing efficiency, we (i) use the exact $V = V(d; d)$ (see \cref{step:commonV} of \autoref{algo:DGP}) to test on $\mathcal{A}_p(0)$ according to \autoref{thm:multi_eig.Est}, and (ii) use the optimized $\widehat{V}$ from algorithm '\textit{(W)JDTE}' and implement both \autoref{thm:multi_eig.Est} and \autoref{cor:multi}. We present the test power versus $\SNR$ and sample size $n$ through histogram plots; see \autoref{fig:multi}.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
    \centering
    \centerline{\input{Plots/PvalueMulti.tikz}}
    \caption{The histograms of p-values for the multi-sample test. We randomly sampled $200$ independent replicates for each data-generating setting of $(\SNR, n)$. The first row of plots are the test results of \autoref{thm:multi_eig.Est} on $\mathcal{A}_p(0)$ supposing the exact common eigenvectors matrix $V$ is known, hence p-values are only available with $\SNR = \infty$. The simulations of the next two rows use the estimated $\widehat{V}$ from '\textit{(W)JDTE}' as input for \autoref{thm:multi_eig.Est} and \autoref{cor:multi}, respectively.}
   \label{fig:multi}
\end{figure}

With evidence that the p-values are almost uniformly distributed by using exact $V$, the test of \autoref{thm:multi_eig.Est} is shown to be effective when the supplied common eigenvectors are sufficiently accurate. However, for estimated $V$, one can observe that for small p-values the proportion of rejections exceeds the nominal test size of $0.05$ for the chi-squared test in \autoref{thm:multi_eig.Est}. While the gamma test in \autoref{cor:multi} admits almost the opposite behavior subceeding the nominal test level. From a theoretical perspective that might also be due to the singular sensitivities when inverting the estimated covariance matrices to calculate the test statistic in \autoref{thm:multi_eig.Est}.
%a test size higher than the nominal level

%However, without numerical precision guarantees for $\widehat{V}$ from '\textit{(W)JDTE}', there is lack of robustness to rely our test on \autoref{thm:multi_eig.Est} despite the uniform p-values in \autoref{fig:multi}, not to mention the singular sensitivities when inverting the estimated covariance matrices.
As for \autoref{cor:multi}, it is a reasonably efficient test method as the false positive rate (type I error) remains at a relatively low level regardless of the sample size $n$, while the false negative rate (type II error) is following a reasonable pattern that with higher $\SNR$, i.e. less perturbations to the shared eigenvectors, the test is ultimately able to reject the $H_0$ as the sample size $n$ increases to be sufficiently large.

\vspace{-0.5cm}
\subsection{Partial test} \label{subse:partialtest}
\vspace{-0.3cm}
For the simulations regarding the partial test in \autoref{sec:part}, we fix $k<d$, and generate the mean matrices as $\mathcal{M}_p(\rho, k; d)$ following \autoref{algo:DGP}. Consistent estimates $\mathcal{A}_p(\rho)$ are obtained similarly from the average of the random observations to follow ordinary CLT. We set $d = 4$, $p = 8$, $k = 2$, $\SNR = \frac{1}{\rho^2} \in \{10, 100, 1000, \infty\}$, and sample size $n \in \{10^2, 10^3, 10^4\}$.

\begin{figure}[htb] %  figure placement: here, top, bottom, or page
    \centering
    \centerline{\input{Plots/PvaluePartial.tikz}}
    \caption{The histograms of p-values for the partial test. $200$ independent replicates are sampled for each data-generating setting of $(\SNR, n)$. Testing with estimated $\widehat{V}$, the first and second rows are results from \autoref{thm:part.Est} and \autoref{cor:partial}, respectively.}
   \label{fig:partial}
\end{figure}

The histograms in \autoref{fig:partial} show that both \autoref{thm:part.Est} and \autoref{cor:partial} have similar testing power. In addition, even if $\SNR$ increases, i.e. the perturbation on common eigenvectors $V$ becomes subtle, the type II error can maintain at almost zero even with small sample size $n$.

As pointed out in \autoref{sec:part}, we assume that the number of partial common eigenvectors in known. Since this assumption is not feasible in practice, we propose a sequential testing procedure. We refer to {\color{cyan} Section A.2} in the supplementary material for a detailed description of the testing procedure and a corresponding simulation study to access its performance.

\vspace{-0.5cm}
\section{Applications} \label{sec:app}
\vspace{-0.3cm}
In \autoref{se:VAR} we consider VAR models and analyze their dynamic structure in terms of our test methods. 
%In particular, we present how our test can be utilized to decouple a VAR model into multiple univariate time series. 
In \autoref{se:Markov}, we introduce the application of our partial test on identical stationary distributions of different Markov chains.

\vspace{-0.5cm}
\subsection{VAR models} \label{se:VAR}
\vspace{-0.3cm}
%A $d$-dimensional time series $\{\bm{y}_t\}_{t=1}^{n+1}$ follows a VAR$(1)$ model if
%\begin{equation}\label{eqn:times}
%\bm{y}_t = \Phi \bm{y}_{t-1} + \bm{e}_t,
%\end{equation}
%with $\Phi \in \mathbb{R}^{d \times d}$, $\bm{y}_t \in \mathbb{R}^d$, and the error series $\bm{e}_t \in \mathbb{R}^d$ follows a white noise process $WN(0, \Sigma_e)$ for some covariance matrix $\Sigma_e$. The coefficient matrices $\Phi$ can be estimated consistently by ordinary least square method; see \cite{Lutkepol2005}.

%In the VAR model \eqref{eqn:times}, the square coefficient matrices $\Phi$ is not necessarily symmetric. In practice, however, such matrix does have restraints. If the determinant of the corresponding lag polynomial
%$$
%h(\lambda) = I_d - \lambda \Phi
%$$
%has roots greater than $1$ in module, the VAR model is stationary.

%Once the joint diagonalization of the matrices $\Phi_i$ is verified, there exists an invertible matrix
%$$
%V = (V_1', \dots, V_d')', \text{ such that } V \Phi_i V^{-1} = D_i = \text{diag}(D_{i1}, \dots, D_{id}),
%$$
%where $D_{ij}$, $j=1,\dots,d$, are the eigenvalues of the matrix $\Phi_i$. The multivariate time series $\bm{y}_t$ in \eqref{eqn:times} can then be linearly transformed as
%$$
%\bm{z}_t = V \bm{y}_t = (V_1 \bm{y}_t, \dots, V_d \bm{y}_t)' =: (\bm{z}_t[1], \dots, \bm{z}_t[d])',
%$$
%such that
%$$
%\bm{z}_t = \sum_{i=1}^{p} D_i \bm{z}_{t-i} + V \bm{e}_t, ~ t = p + 1, \dots, n+1
%$$
%and
%$$
%\mathbb{E}[\bm{z}_t[j]] = \mathbb{E}[V_j \bm{y}_t] = \sum_{i = 1}^p D_{ij} \mathbb{E}[V_j \bm{y}_{t-i}] = \sum_{i = 1}^p D_{ij} \mathbb{E}[\bm{z}_{t - i}[j]].
%$$
%Concluding, given simultaneously diagonalizable coefficient matrices, a VAR model can indeed be decoupled into $d$ time series.
%As studied in \cite{chishti}, macroeconomic indices can be assumed to follow a VAR model of order one (VAR(1)).
We consider gross domestic product (GDP), money supply (M2), and real effective exchange rate (REER) for eight of the most influential countries distributed across three different continents; see \Cref{fig:VAR1_eg}. GDP and M2 data are available through \cite{ceic}, and REER data through \cite{oecd}. The Bayesian information criterion (BIC) favors VAR models of order one for all eight countries. Therefore, for each country, we fit 
$$
\bm{y}_{t}^c = \bm{\mu}^c + \Phi^c \bm{y}_{t - 1}^c + \bm{e}_{t}^c,
\hspace{0.2cm} t = 1, \dots, n+1,
$$
to the three different economic indices (GDP, M2, REER),
where the superscript $c$ distinguishes the eight countries.
We may compare the growth tendency among subjects if the eigendecomposition guarantees common components. 
%For the setting of different VAR(1) models of distinct but similar subjects, we consider different macroeconomic indices. For example, similar to \cite{chishti}, we consider gross domestic product (GDP), money supply (M2), and real effective exchange rate (REER) index as a part of the VAR models for 8 most influential countries distributed across three different continents. GDP and M2 data are available through \cite{ceic}, and REER data through \cite{oecd}. With superscript $c$ distinguishing countries, we state the VAR(1) models as
If our test fails to reject the null hypothesis for the matrices $\Phi^c$, it provides us confidence to conduct the transformation $\bm{z}_{t}^c = V \bm{y}_{t}^c$, $\bm{\mu}_*^c = V \bm{\mu}^c$ with $V \Phi^c V^{-1} = D^c$ diagonal. The lagged cross-dependence cancels out for each coordinate of $\bm{z}_{t}^c$, i.e.
$\mathbb{E}[\bm{z}_t^c] = \bm{\mu}_*^c + D^c \mathbb{E}[\bm{z}_{t-1}^c]$.
Hence, with $\bm{z}^{c}_t$ as the new set of variables, comparison between countries can then be done on each variable separately.

%Note that the independence of elements between estimated matrices is reasonable if they are calculated from VAR models at different time lags or distinct subjects, at least asymptotically.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
    \centering
    \centerline{\input{Plots/ts.tikz}}
    \caption{The time plots of the macro-economic indices of eight countries from the first quarter of 1992 to the first quarter of 2020. The data set is collected from \cite{ceic} and \cite{oecd}.}
    \label{fig:VAR1_eg}
\end{figure}

The quarterly data of seasonally adjusted time series span from the first quarter of 1992 to the first quarter of 2020, with length $n+1 = 113$, and are pre-processed by taking the log-difference and standardization. The least square estimators of the coefficient matrices are then obtained, and they follow asymptotic normal distributions with rate $n^{-1/2}$; see Chapter 3 in \cite{Lutkepol2005} for more details on estimating VAR models. We conduct our multi-sample tests \autoref{cor:multi} and \autoref{thm:multi_eig.Est} on those coefficient matrices.

Implementing \autoref{cor:multi}, we get a p-value 0.991 which indicates that we fail to reject $H_0$ concerning all eight countries. Meanwhile \autoref{thm:multi_eig.Est} yields a p-value of 0.045 at the margin of rejection level. 
These results are not surprising based on our simulation study in \autoref{se:sim-multi}. \autoref{fig:multi} indicates that the chi-squared test (\autoref{thm:multi_eig.Est}) tends to reject the hypothesis more often than suggested by the nominal test size and the gamma test (\autoref{cor:multi}) tends to reject the hypothesis less often.

In addition, if we look at the p-value table (\autoref{fig:pairP}) for simultaneous commutator-based tests according to \autoref{thm:comm.Est}, we see that the United States and China both share quite evident similarities with all other countries, and there are reasonable similarity structures within each continental group, except between Korea and Japan. Splitting the eight countries into three continental groups and repeating our test based on \autoref{cor:multi} group-wisely, we successfully get the conclusion that $H_0$ holds within the continental groups. The corresponding p-values are, 0.530 for North America (United States and Canada), 0.978 for Europe (France, Germany, and United Kingdom), and 0.900 for Asia (China, Japan, and Korea). In addition, even the unstable \autoref{thm:multi_eig.Est} gives p-values 0.076 and 0.657 for North America and Europe, respectively.

\begin{figure}[htb]
    \centering
    \centerline{\input{Plots/pvalMat.tikz}}
    \caption{The heatmap of the p-values of the pairwise commutator-based test.}
    \label{fig:pairP}
\end{figure}

Since the test statistic in \autoref{cor:multi} gives a relatively large p-value and taking into consideration the test results of our pairwise test as well as the test performances in the simulation study, we may conclude that one fails to reject $H_0$. Our test results provide evidence that it is reasonable to assume that the coefficient matrices share the same eigenvectors. The approximated common left eigenvectors $V$ and the new shared variables are
\begin{equation}
    \bm{z}_t = V  \begin{pmatrix} \mbox{GDP}_t \\ \mbox{M2}_t \\ \mbox{REER}_t \end{pmatrix} = \begin{pmatrix} 0.66 & 0.41 & -0.10 \\
    0.62 & 0.04 & -0.81\\
    0.72 & -1.03 & -0.10\end{pmatrix} \begin{pmatrix} \mbox{GDP}_t \\ \mbox{M2}_t \\ \mbox{REER}_t \end{pmatrix}.\label{eqn:Vinv}
\end{equation}

In addition, we conduct the partial test for one common eigenvector ($k=1$) on all eight coefficient matrices, and get that the p-values equal 0.737 from the chi-squared test \eqref{eqn:partchi} and 0.964 from the gamma approximation \eqref{eqn:part_approx}. In addition, when considering $k = 2$, \eqref{eqn:part_approx} gives the p-value 0.948. Recall that the transformed variables have the following notation $\bm{z}_t^c = (\bm{z}_t[1], \bm{z}_t[2], \bm{z}_t^c[3])$, where the first two variables (without superscript $c$) are shared across all eight countries. The two common variables $\bm{z}_t[1]$ and $\bm{z}_t[2]$ that only depend on itself in expectation are
\begin{align*}
    \bm{z}_t[1] & = 0.67 * \mbox{GDP}_t + 0.34 * \mbox{M2}_t - 0.68 * \mbox{REER}_t,\\
    \bm{z}_t[2] & = 0.58 * \mbox{GDP}_t - 0.93 * \mbox{M2}_t - 0.03 * \mbox{REER}_t.
\end{align*}
Note that $\bm{z}_t[1]$ and $\bm{z}_t[2]$ correspond to the first and third row of $V$ in \eqref{eqn:Vinv} respectively, with a bit rescaling and fluctuations.

\vspace{-0.5cm}
\subsection{Stationary distribution of Markov chains}\label{se:Markov}
\vspace{-0.3cm}
Consider $p$ recurrent Markov chains $\{X_i = \{X_{i,t}\}_{t = 1}^{n+1} \}_{i = 1}^p$ with time length $n+1$ and labels $X_{i,t}$ from a finite discrete state space $\{1, \dots, d\}$. Within each chain the transition probability matrix $P_i$ can be estimated consistently to follow asymptotic normality. % possibly via maximum likelihood estimation.
For example, $P_i$ can be estimated by
\begin{equation}
    \widehat{P}_i = \big(\widehat{P}_{i,rs}\big)_{d \times d}, ~ \widehat{P}_{i,rs} = \frac{\sum_{t=1}^{n} \mathbb{I}(X_{i,t} = r) \mathbb{I}(X_{i,t+1} = s)}{\sum_{t = 1}^{n} \mathbb{I}(X_{i,t} = r)};\label{eqn:P_est}
\end{equation}
see (3) in \cite{barsotti}.
%Lemma 3.1 in Barsotti
The estimates $\widehat{P}_i$ in \eqref{eqn:P_est} can be deduced to follow asymptotic normality in the sense of \autoref{ass:normal} with limiting covariance $\Sigma_i$ given by
\begin{equation}
\lim_{n \to \infty} n \cov(\widehat{P}_{i,rs}, \widehat{P}_{i,uv}) = 
\begin{cases}
P_{i,rs} (1 - P_{i,rs})/\bm{\pi}_{i,r}, & r = u, s = v,
\\
-P_{i,rs} P_{i,uv}/\bm{\pi}_{i,r}, & r = u, s \neq v,
\\
0, & \mbox{otherwise,}
\end{cases} \label{eqn:covMarkov}
\end{equation}
where $\bm{\pi}_i = (\bm{\pi}_{i,1}, \dots, \bm{\pi}_{i,d}) \in \mathbb{R}^d$ is the stationary distribution of chain $X_i$; see Lemma 3.1 in \cite{barsotti}. Note that $\bm{\pi}_{i,r}$ is strictly positive for any $i$ and $r$ since all chains are recurrent. In practice, $\bm{\pi}_i$ can be estimated either from $\widehat{P}_i$ or directly from chain $X_i$.

Applying the Perron-Frobenius theorem \citep[Theorem 8.4.4]{Horn}, it is possible to conduct our partial test to check identical stationary distributions if we could find a common non-negative eigenvector. Using the fact that each $P_i$ (and $\widehat{P}_i$) has stationary distribution as an eigenvector corresponding to eigenvalue $1$, we can optimize the common non-negative eigenvector associated with fixed eigenvalue $1$, which must be proportional to the common distribution vector if it exists. We aim to find the non-negative vector $\bm{\widehat{\pi}}$ which optimizes the problem
\begin{multline}\label{eqn:minimize}
    \mbox{minimize } f(\bm{x}) = \sum_{i = 1}^p \bm{x}'(\widehat{P}_i - I_d)(\widehat{P}_i' - I_d)\bm{x},\\
    ~ \mbox{subject to } \sum_{i=1}^d \bm{x}_i = 1, \mbox{ and }\bm{x}_i \ge 0, ~ \forall i = 1, \dots, d.
\end{multline}
It can be solved via quadratic programming with explicitly given constraints.

\begin{figure}[H] %  figure placement: here, top, bottom, or page
    \centering
    \centerline{\input{Plots/Streamflow.tikz}}
    \caption{The discharge dataset for Hudson river with different time resolutions and disjoint time coverage. The unit is $\mbox{ft}^3/s$. The discharges are classified as 'drought' if in the lower region (below 25 percentile), as 'flooding' if in the upper region (above 75 percentile), and as 'normal' if in the middle. In the plot, the (colored) crosses represent the classified states, with axis on the right.}
   \label{fig:stream}
\end{figure}

We consider the streamflow discharge data of Hudson river collected at Fort Edward NY; see \autoref{fig:stream}. The dataset is available at \cite{usgs}. We use the historical weekly data (from Oct. 1st 1979 to Sept. 30th 2014) and the more recent daily data (from Oct. 1st 2015 to Sept. 30th 2020). Both series have the same length $n = 1827$. We classify the discharge records to 3 levels according to the percentile statistics based on data from Oct. 1st 1977 to Sept. 30th 2019. An observation is referred to as 'drought' if it is below the 25th percentile, as 'flooding' if above the 75th percentile, and as 'normal' otherwise. We assume that this 3-state sequence satisfies the Markov property, and estimate the transition probability matrices (see \autoref{fig:streamTran}) for our partial test. Note that the two time series are disjoint, hence it is reasonable to view these estimators as independent.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
    \centering
    \centerline{\input{Plots/tranMat.tikz}}
    \caption{The 2 transition matrices are estimated according to \eqref{eqn:P_est} and they are statistically different according to asymptotic Wald test. Numerically speaking, at least the diagonal entries of the 2 matrices are obviously unequal.}
   \label{fig:streamTran}
\end{figure}

We implement the partial test according to both \autoref{thm:part.Est} and \autoref{cor:partial}, with parameter $k = 1$ and $V = \bm{\widehat{\pi}}_i$ given by optimization \eqref{eqn:minimize}. The two tests give the p-values of 0.106 and 0.102 respectively, which indicate that one fails to reject $H_0^{*}$ in \eqref{hyp:part} and may conclude the two series share the same stationary distribution. The estimated common stationary distribution is
$
\mathbb{P}(\text{Drought}) = 0.230, \hspace{0.15cm} \mathbb{P}(\text{Normal}) = 0.519, \hspace{0.15cm} \mathbb{P}(\text{Flooding}) = 0.251.
$
The results indicate that for time series with different time resolutions, even though the transition probability matrices can be different, the stationary distribution might still be verified to be statistically equivalent.

\vspace{-0.5cm}
\section{Conclusions}\label{sec:con}
\vspace{-0.3cm}
In this work, we focused on general asymmetric matrix-valued population quantities and developed effective tests on simultaneous diagonalizability under both two-sample and multi-sample settings. We also generalized our designs to test on partially shared eigenvectors, with introduction of a supplemental optimization algorithm that retrieves those common eigenvectors. 
%Our simulation study indicated reliable performance of the tests. 
%Furthermore, we proposed novel applications for simultaneous diagonalization of possibly asymmetric matrices. 
Finally, we applied our test to real data examples and successfully revealed interesting structural properties of the underlying models.
%We believe that our results provide a new perspective on the usefullness of 

In this work, we considered the classical ``fixed $d$, large $n$ regime. However, many contemporary data go beyond the low dimensional setting and require the dimension $d$ to be of the same order as, or possibly even larger than, the sample size $n$.
While the high-dimensional setting goes beyond the scope of this work, we added a discussion and simulation study in {\color{cyan} Section A.3} of the supplementary material to emphasize that the methodology in this paper is not sufficient to do testing on high-dimensional data.

Other possible future directions include adjacency matrices of weighted directed networks which are possibly asymmetric square matrices. Our partial test could be of special interest to test on their leading eigenvectors. %which give information whether the PageRank algorithm, which was first introduced by \cite{page}, has the same output for all samples. 
Combined with the high-dimensional setting, one could even study expanding networks.

In our multi-sample (\autoref{sec:multi}) and partial tests (\autoref{sec:part}), the estimated $\widehat{V}$ is treated as given and deterministic in the test statistics. It might be possible to pursue more rigorous results by deriving the stochastic properties of the optimizer $\widehat{V}$. 



\noindent
\textbf{Supplementary material} The supplementary material includes all appendices. In particular, it includes an alternative approach for the two-sample test, all proofs and some additional discussions.

\noindent
\textbf{Acknowledgments} The authors thank the anonymous Reviewer and the Associate editor for their comments which helped improving the paper. 
The authors gratefully acknowledge financial support from a Xerox PARC Faculty Research Award, National Science Foundation Awards 1455172, 1934985, 1940124, and 1940276, USAID, and Cornell University Atkinson Center for a Sustainable Future.
This research was conducted with support from the Cornell University Center for Advanced Computing, which receives funding from Cornell University, the National Science Foundation, and members of its Partner Program.

\noindent
\textbf{Conflict of interest} The authors report there are no competing interests to declare.








\singlespacing
\small
%\bibliographystyle{Chicago}
\bibliographystyle{plainnat}
% \bibliographystyle{plain}
\bibliography{main}

\end{document}
